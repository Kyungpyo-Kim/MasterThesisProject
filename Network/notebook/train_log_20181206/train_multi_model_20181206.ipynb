{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Log 20181206\n",
    "\n",
    "* Add multi-model training structure\n",
    "\n",
    "## Results\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "## Trained model\n",
    "* [Download link]()\n",
    "\n",
    "## Evaluation\n",
    "* Incorrect sample\n",
    "![results]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\" Path \"\"\"\n",
    "data_train_path = os.path.abspath('../../../new_dataset/dataset/dataset_20181203_01/train.h5')\n",
    "data_vali_path = os.path.abspath('../../../new_dataset/dataset/dataset_20181203_01/vali.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\" Load dataset\"\"\"\n",
    "\n",
    "data = []\n",
    "class_label = []\n",
    "heading_label = []\n",
    "\n",
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    class_label = f['class'][:]\n",
    "    heading_label = f['heading'][:]\n",
    "    return (data, class_label, heading_label)\n",
    "\n",
    "data_train, class_label_train, heading_label_train = load_h5(data_train_path)\n",
    "data_vali, class_label_vali, heading_label_vali = load_h5(data_vali_path)\n",
    "\n",
    "data.append(data_train)\n",
    "data.append(data_vali)\n",
    "class_label.append(class_label_train)\n",
    "class_label.append(class_label_vali)\n",
    "heading_label.append(heading_label_train)\n",
    "heading_label.append(heading_label_vali)\n",
    "\n",
    "\"\"\" Data statistics \"\"\"\n",
    "\n",
    "label_list = [0,1,2]\n",
    "\n",
    "y_val = []\n",
    "for i in range( len ( data) ):\n",
    "    for j in range ( len ( label_list ) ):\n",
    "        y_val.append(np.sum(class_label[i] == label_list[j]))\n",
    "\n",
    "x_name=('unknown-train', 'cars-train','pedestrian-train',\n",
    "        'unknown-vali', 'cars-vali', 'pedestrian-vali')\n",
    "\n",
    "index = range( len(x_name) )\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(index, y_val, tick_label=x_name, align='center')\n",
    "plt.ylabel('Number of dataset')\n",
    "plt.title('Label distribution')\n",
    "plt.xlim( -1, len(x_name))\n",
    "plt.ylim( 0, np.max(y_val) * 1.1 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n",
      "\n",
      "\n",
      " Train one epoch   1 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-73784573fa59>\", line 255, in run_training\n",
      "    ops['pred']], feed_dict=feed_dict)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 889, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "ResourceExhaustedError: OOM when allocating tensor with shape[1024,512]\n",
      "\t [[Node: gradients/fc1/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Reshape, gradients/fc1/BiasAdd_grad/tuple/control_dependency)]]\n",
      "\t [[Node: Adam/update_transform_net2/transform_feat/weights/ApplyAdam/_366 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5472_Adam/update_transform_net2/transform_feat/weights/ApplyAdam\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n",
      "Caused by op u'gradients/fc1/MatMul_grad/MatMul_1', defined at:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 1073, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-73784573fa59>\", line 365, in <module>\n",
      "    p.start()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 130, in start\n",
      "    self._popen = Popen(self)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/forking.py\", line 126, in __init__\n",
      "    code = process_obj._bootstrap()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-73784573fa59>\", line 123, in run_training\n",
      "    train_op = optimizer.minimize(loss, global_step=batch)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 343, in minimize\n",
      "    grad_loss=grad_loss)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 414, in compute_gradients\n",
      "    colocate_gradients_with_ops=colocate_gradients_with_ops)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n",
      "    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n",
      "    return grad_fn()  # Exit early\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n",
      "    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_grad.py\", line 922, in _MatMulGrad\n",
      "    grad_b = math_ops.matmul(a, grad, transpose_a=True)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\n",
      "    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\n",
      "    name=name)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "...which was originally created as op u'fc1/MatMul', defined at:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "[elided 22 identical lines from previous traceback]\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-73784573fa59>\", line 93, in run_training\n",
      "    pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
      "  File \"/home/gozilla/kyungpyo/git/MasterThesisProject/Network/model/model_out_5_heading.py\", line 68, in get_model\n",
      "    scope='fc1', bn_decay=bn_decay)\n",
      "  File \"/home/gozilla/kyungpyo/git/MasterThesisProject/Network/model/utils/tf_util.py\", line 326, in fully_connected\n",
      "    outputs = tf.matmul(inputs, weights)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 1891, in matmul\n",
      "    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2437, in _mat_mul\n",
      "    name=name)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/gozilla/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1024,512]\n",
      "\t [[Node: gradients/fc1/MatMul_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Reshape, gradients/fc1/BiasAdd_grad/tuple/control_dependency)]]\n",
      "\t [[Node: Adam/update_transform_net2/transform_feat/weights/ApplyAdam/_366 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5472_Adam/update_transform_net2/transform_feat/weights/ApplyAdam\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import multiprocessing\n",
    "# import time , datetime\n",
    "# import numpy as np\n",
    "\n",
    "# sys.path.append( os.path.abspath('../../../Dataset/scripts'))\n",
    "# from utils import *\n",
    "\n",
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(20000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 10\n",
    "    \n",
    "    HEADING_LOSS_WEIGHT = 0.1\n",
    "\n",
    "    \n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" \n",
    "    Load traing model \n",
    "    \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points, h_reg_weight=HEADING_LOSS_WEIGHT)\n",
    "            \n",
    "#             classify_loss = MODEL.get_classify_loss(pred, class_labels_pl)\n",
    "#             mat_diff_loss = MODEL.get_classify_loss(pred, end_points)\n",
    "#             heading_loss = MODEL.get_classify_loss(pred, heading_labels_pl)\n",
    "                        \n",
    "            tf.summary.scalar('loss', loss)\n",
    "#             tf.summary.scalar('classify_loss', classify_loss)\n",
    "#             tf.summary.scalar('mat_diff_loss', mat_diff_loss)\n",
    "#             tf.summary.scalar('heading_loss', heading_loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "#                'classify_loss': classify_loss,\n",
    "#                'mat_diff_loss': mat_diff_loss,\n",
    "#                'heading_loss': heading_loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            print \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) \n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            class_results = []\n",
    "            heading_results = []\n",
    "            \n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            num_batches = 10\n",
    "            num_batches = 500\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, cls_loss, mat_loss, hea_loss, pred_val = sess.run([ops['merged'], \n",
    "                                                                                               ops['step'],\n",
    "                                                                                               ops['train_op'], \n",
    "                                                                                               ops['loss'], \n",
    "#                                                                                                ops['classify_loss'],\n",
    "#                                                                                                ops['mat_diff_loss'],\n",
    "#                                                                                                ops['heading_loss'],\n",
    "                                                                                               ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                pred_val = sess.run([ops['pred']], feed_dict=feed_dict )\n",
    "        \n",
    "                # heading\n",
    "                heading_results.extend(pred_val[0][:,3])\n",
    "\n",
    "                pred_val_class = np.argmax(pred_val[0][:,:3], 1)\n",
    "                class_results.extend(pred_val_class)\n",
    "                \n",
    "#             print \"---------------------------------------\"\n",
    "#             print dataset_list[i]\n",
    "#             print \"---------------------------------------\"\n",
    "\n",
    "#             prediction = np.array(class_results_list[i])\n",
    "#             ground_truth = np.array(class_label[i][:len(prediction)])\n",
    "\n",
    "#             cm = confusion_matrix(ground_truth, prediction)\n",
    "\n",
    "#             label_list = ['unknown', 'car', 'pedes']\n",
    "#             print(classification_report(ground_truth, prediction, target_names=label_list))\n",
    "\n",
    "            \n",
    "#             plot_confusion_matrix(cm, normalize = False, title='Confusion matrix', cmap=plt.cm.Oranges, label_list = label_list)\n",
    "#             plt.show()\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            is_training = False\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "                   \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            num_batches = 10\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, cls_loss, mat_loss, hea_loss, pred_val = sess.run([ops['merged'], \n",
    "                                                                                          ops['step'],\n",
    "                                                                                          ops['loss'], \n",
    "#                                                                                           ops['classify_loss'],\n",
    "#                                                                                           ops['mat_diff_loss'],\n",
    "#                                                                                           ops['heading_loss'],                                                                                               \n",
    "                                                                                          ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                   \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5_4',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
