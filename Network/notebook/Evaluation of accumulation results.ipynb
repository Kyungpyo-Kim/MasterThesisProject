{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of accumulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import pcl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "sys.path.append(os.path.abspath('../../Dataset/scripts') )\n",
    "from utils import *\n",
    "\n",
    "sys.path.append( os.path.abspath('../model') )\n",
    "from train import *\n",
    "import model as MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parameters \"\"\"\n",
    "GPU_INDEX = 0\n",
    "NUM_POINT = 256\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# model_path = os.path.abspath('../notebook/train_log_20181020/model/backup/model.ckpt')\n",
    "model_path = os.path.abspath('../notebook/train_log_20181020/model/model.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386/386 [00:00<00:00, 582.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0 0\n",
      "1 234\n",
      "2 98\n",
      "3 24\n",
      "4 16\n",
      "5 10\n",
      "6 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load data \"\"\"\n",
    "\n",
    "data_path = os.path.abspath('../../Dataset/evaluation_acc_cls')\n",
    "\n",
    "pcd_file_list = []\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    rootpath = os.path.join(os.path.abspath(data_path), root)\n",
    "\n",
    "    for file in files:\n",
    "        if file.split('.')[-1] == 'pcd':\n",
    "\n",
    "            filepath = os.path.join(rootpath, file)\n",
    "            pcd_file_list.append(filepath)\n",
    "\n",
    "af_frame = 6\n",
    "\n",
    "data = [[] for _ in range(af_frame + 1)]\n",
    "acc_frame_list = []\n",
    "\n",
    "## Regular expression\n",
    "re_af = re.compile(\"AF_\\d{4}\")\n",
    "\n",
    "for pcd_file in tqdm(pcd_file_list):\n",
    "    cloud = np.asarray(pcl.load(pcd_file))\n",
    "    \n",
    "    mat_results = re_af.search(pcd_file)\n",
    "    af =  int( mat_results.group(0).split('_')[-1] )\n",
    "    \n",
    "    if af <= 6:\n",
    "        data[af].append(cloud)\n",
    "        \n",
    "sys.stdout.flush()\n",
    "print len(data)\n",
    "for i in range( len(data) ): print i, len(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data visualization \"\"\"\n",
    "%matplotlib qt\n",
    "\n",
    "acc_frame_sel = 5\n",
    "idx = 0\n",
    "\n",
    "start_idx = (6 * 6) * idx\n",
    "end_idx = start_idx + 6*6\n",
    "\n",
    "clouds = data[acc_frame_sel]\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure()\n",
    "\n",
    "fig.subplots_adjust(left=0.02,top= 0.98,bottom=0.02,right=0.98,wspace=0.1,hspace=0.5)\n",
    "\n",
    "ax=fig.add_subplot(111, projection='3d')\n",
    "ax = display_point_cloud_box_ax(ax, clouds[idx]) \n",
    "\n",
    "point_num = clouds[idx].shape[0]\n",
    "\n",
    "ax.set_title('idx: {} point_num: {}'.format(idx, point_num))\n",
    "\n",
    "figManager = plt.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from /media/kyungpyo/SmartCarContest/MasterThesisProject/Network/notebook/train_log_20181020/model/model.ckpt\n",
      "Model restored\n",
      "Dataset 1 / 7 \n",
      "Number of total dataset / evaluated dataset: 0 / 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2 / 7 \n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "Number of total dataset / evaluated dataset: 234 / 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 234/234 [00:06<00:00, 34.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3 / 7 \n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "Number of total dataset / evaluated dataset: 98 / 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 98/98 [00:02<00:00, 41.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 4 / 7 \n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "Number of total dataset / evaluated dataset: 24 / 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 24/24 [00:00<00:00, 32.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 5 / 7 \n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "Number of total dataset / evaluated dataset: 16 / 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 16/16 [00:00<00:00, 25.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 6 / 7 \n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "Number of total dataset / evaluated dataset: 10 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 26.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 7 / 7 \n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "(256, 3)\n",
      "Number of total dataset / evaluated dataset: 4 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 24.25it/s]\n"
     ]
    }
   ],
   "source": [
    "is_training = False\n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "    pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "    is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "    # simple model\n",
    "    pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl)\n",
    "    loss = MODEL.get_loss(pred, labels_pl, end_points)\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "# Create a session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "config.log_device_placement = True\n",
    "\n",
    "ops = {'pointclouds_pl': pointclouds_pl,\n",
    "       'labels_pl': labels_pl,\n",
    "       'is_training_pl': is_training_pl,\n",
    "       'pred': pred,\n",
    "       'loss': loss}\n",
    "\n",
    "# ops.reset_default_graph() \n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Restore variables from disk.\n",
    "saver.restore(sess, model_path)\n",
    "\n",
    "print \"Model restored\"\n",
    "sys.stdout.flush()\n",
    "\n",
    "results_list = []\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    print \"Dataset {} / {} \".format(i+1, len(data))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    cd_list = []\n",
    "    for d in data[i]:\n",
    "        cd = NormalizeResample(d, NUM_POINT)\n",
    "        cd_list.append(cd)\n",
    "        \n",
    "    current_data = np.asarray(cd_list)\n",
    "        \n",
    "    current_label = np.array([0 for _ in range( current_data.shape[0] )])\n",
    "    \n",
    "    num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "    \n",
    "    print \"Number of total dataset / evaluated dataset: {} / {}\".format(current_data.shape[0], num_batches * BATCH_SIZE)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for batch_idx in trange(num_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "        feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx].reshape((BATCH_SIZE, NUM_POINT, 3)),\n",
    "                     ops['labels_pl']: current_label[start_idx:end_idx].reshape(BATCH_SIZE),\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        loss_val, pred_val = sess.run([ops['loss'], ops['pred']], feed_dict=feed_dict )\n",
    "\n",
    "        pred_val = np.argmax(pred_val, 1)\n",
    "        results.extend(pred_val)\n",
    "    \n",
    "    results_list.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aae0730a1f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnp_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_re\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mnp_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_list' is not defined"
     ]
    }
   ],
   "source": [
    "for i, results in enumerate(results_list):\n",
    "    print len(results)\n",
    "    np_re = np.array(results)\n",
    "    if np_re.shape[0] == 0: continue\n",
    "    print i, float ( np.sum(np_re == np.ones(np_re.shape[0])) ) / float ( np_re.shape[0] ) * 100.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
