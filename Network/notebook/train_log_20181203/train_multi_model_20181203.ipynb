{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Log 20181201\n",
    "\n",
    "* Add multi-model training structure\n",
    "\n",
    "## Results\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "## Trained model\n",
    "* [Download link]()\n",
    "\n",
    "## Evaluation\n",
    "* Incorrect sample\n",
    "![results]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time , datetime\n",
    "\n",
    "# sys.path.append( os.path.abspath('../../../Dataset/scripts'))\n",
    "# from utils import *\n",
    "\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "from train import *\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Path \"\"\"\n",
    "data_train_path = os.path.abspath('../../../new_dataset/dataset/dataset_20181203_01/train.h5')\n",
    "data_vali_path = os.path.abspath('../../../new_dataset/dataset/dataset_20181203_01/vali.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Load dataset\"\"\"\n",
    "\n",
    "data = []\n",
    "class_label = []\n",
    "heading_label = []\n",
    "\n",
    "def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    class_label = f['class'][:]\n",
    "    heading_label = f['heading'][:]\n",
    "    return (data, class_label, heading_label)\n",
    "\n",
    "data_train, class_label_train, heading_label_train = load_h5(data_train_path)\n",
    "data_vali, class_label_vali, heading_label_vali = load_h5(data_vali_path)\n",
    "\n",
    "data.append(data_train)\n",
    "data.append(data_vali)\n",
    "class_label.append(class_label_train)\n",
    "class_label.append(class_label_vali)\n",
    "heading_label.append(heading_label_train)\n",
    "heading_label.append(heading_label_vali)\n",
    "\n",
    "\"\"\" Data statistics \"\"\"\n",
    "\n",
    "label_list = [0,1,2]\n",
    "\n",
    "y_val = []\n",
    "for i in range( len ( data) ):\n",
    "    for j in range ( len ( label_list ) ):\n",
    "        y_val.append(np.sum(class_label[i] == label_list[j]))\n",
    "\n",
    "x_name=('unknown-train', 'cars-train','pedestrian-train',\n",
    "        'unknown-vali', 'cars-vali', 'pedestrian-vali')\n",
    "\n",
    "index = range( len(x_name) )\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(index, y_val, tick_label=x_name, align='center')\n",
    "plt.ylabel('Number of dataset')\n",
    "plt.title('Label distribution')\n",
    "plt.xlim( -1, len(x_name))\n",
    "plt.ylim( 0, np.max(y_val) * 1.1 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_3:0\", shape=(), dtype=bool, device=/device:GPU:0)\n",
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n",
      "2018-12-03 22:03:49.714966  \n",
      "\n",
      " Train one epoch   1 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:39<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:13:30.194931  [Epoch 0] mean loss: 0.011399\n",
      "2018-12-03 22:13:30.197077  [Epoch 0] heading rmse[deg]: 147.107654\n",
      "2018-12-03 22:13:30.199025  [Epoch 0] class accuracy: 0.770402\n",
      "2018-12-03 22:13:30.215946  [Epoch 0] avg class acc: 0.795778\n",
      "2018-12-03 22:13:30.217748  [Epoch 0] indivisual [0] class recall: 0.708417\n",
      "2018-12-03 22:13:30.219423  [Epoch 0] indivisual [0] class precision: 0.758754\n",
      "2018-12-03 22:13:30.221168  [Epoch 0] indivisual [1] class recall: 0.805585\n",
      "2018-12-03 22:13:30.223081  [Epoch 0] indivisual [1] class precision: 0.765290\n",
      "2018-12-03 22:13:30.224919  [Epoch 0] indivisual [2] class recall: 0.873333\n",
      "2018-12-03 22:13:30.226907  [Epoch 0] indivisual [2] class precision: 0.831922\n",
      "2018-12-03 22:13:30.228612   Evaluation one (validation set) epoch   1 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:14:16.318422  [Epoch 0] Validation mean loss: 0.009211\n",
      "2018-12-03 22:14:16.320021  [Epoch 0] Validation heading rmse[deg]: 106.396026\n",
      "2018-12-03 22:14:16.321475  [Epoch 0] Validation class accuracy: 0.832788\n",
      "2018-12-03 22:14:16.322977  [Epoch 0] Validation avg class acc: 0.213410\n",
      "2018-12-03 22:14:16.324804  [Epoch 0] Validation indivisual [0] class recall: 0.229685\n",
      "2018-12-03 22:14:16.326466  [Epoch 0] Validation indivisual [0] class precision: 0.753937\n",
      "2018-12-03 22:14:16.328205  [Epoch 0] Validation indivisual [1] class recall: 0.181865\n",
      "2018-12-03 22:14:16.329878  [Epoch 0] Validation indivisual [1] class precision: 0.951098\n",
      "2018-12-03 22:14:16.331577  [Epoch 0] Validation indivisual [2] class recall: 0.228679\n",
      "2018-12-03 22:14:16.333231  [Epoch 0] Validation indivisual [2] class precision: 0.846083\n",
      "2018-12-03 22:14:16.334904  \n",
      "\n",
      " Train one epoch   2 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:23:55.767410  [Epoch 1] mean loss: 0.004901\n",
      "2018-12-03 22:23:55.769299  [Epoch 1] heading rmse[deg]: 105.446991\n",
      "2018-12-03 22:23:55.770944  [Epoch 1] class accuracy: 0.911220\n",
      "2018-12-03 22:23:55.772695  [Epoch 1] avg class acc: 0.918323\n",
      "2018-12-03 22:23:55.774255  [Epoch 1] indivisual [0] class recall: 0.890810\n",
      "2018-12-03 22:23:55.775902  [Epoch 1] indivisual [0] class precision: 0.906377\n",
      "2018-12-03 22:23:55.777432  [Epoch 1] indivisual [1] class recall: 0.924049\n",
      "2018-12-03 22:23:55.778970  [Epoch 1] indivisual [1] class precision: 0.923412\n",
      "2018-12-03 22:23:55.780605  [Epoch 1] indivisual [2] class recall: 0.940111\n",
      "2018-12-03 22:23:55.782387  [Epoch 1] indivisual [2] class precision: 0.882733\n",
      "2018-12-03 22:23:55.784019   Evaluation one (validation set) epoch   2 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:24:42.009175  [Epoch 1] Validation mean loss: 0.003617\n",
      "2018-12-03 22:24:42.011048  [Epoch 1] Validation heading rmse[deg]: 104.693826\n",
      "2018-12-03 22:24:42.012853  [Epoch 1] Validation class accuracy: 0.947249\n",
      "2018-12-03 22:24:42.014648  [Epoch 1] Validation avg class acc: 0.232854\n",
      "2018-12-03 22:24:42.016484  [Epoch 1] Validation indivisual [0] class recall: 0.241758\n",
      "2018-12-03 22:24:42.018215  [Epoch 1] Validation indivisual [0] class precision: 0.916366\n",
      "2018-12-03 22:24:42.019978  [Epoch 1] Validation indivisual [1] class recall: 0.235601\n",
      "2018-12-03 22:24:42.021714  [Epoch 1] Validation indivisual [1] class precision: 0.994238\n",
      "2018-12-03 22:24:42.023207  [Epoch 1] Validation indivisual [2] class recall: 0.221204\n",
      "2018-12-03 22:24:42.024884  [Epoch 1] Validation indivisual [2] class precision: 0.895714\n",
      "2018-12-03 22:24:42.026545  \n",
      "\n",
      " Train one epoch   3 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:34:21.662246  [Epoch 2] mean loss: 0.003103\n",
      "2018-12-03 22:34:21.664169  [Epoch 2] heading rmse[deg]: 104.706929\n",
      "2018-12-03 22:34:21.665954  [Epoch 2] class accuracy: 0.947591\n",
      "2018-12-03 22:34:21.667803  [Epoch 2] avg class acc: 0.948351\n",
      "2018-12-03 22:34:21.669551  [Epoch 2] indivisual [0] class recall: 0.933407\n",
      "2018-12-03 22:34:21.671355  [Epoch 2] indivisual [0] class precision: 0.947060\n",
      "2018-12-03 22:34:21.673042  [Epoch 2] indivisual [1] class recall: 0.960647\n",
      "2018-12-03 22:34:21.674653  [Epoch 2] indivisual [1] class precision: 0.961921\n",
      "2018-12-03 22:34:21.676270  [Epoch 2] indivisual [2] class recall: 0.951000\n",
      "2018-12-03 22:34:21.677783  [Epoch 2] indivisual [2] class precision: 0.895293\n",
      "2018-12-03 22:34:21.679357   Evaluation one (validation set) epoch   3 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:35:07.358280  [Epoch 2] Validation mean loss: 0.002244\n",
      "2018-12-03 22:35:07.360177  [Epoch 2] Validation heading rmse[deg]: 104.203926\n",
      "2018-12-03 22:35:07.362006  [Epoch 2] Validation class accuracy: 0.975093\n",
      "2018-12-03 22:35:07.363989  [Epoch 2] Validation avg class acc: 0.239507\n",
      "2018-12-03 22:35:07.365653  [Epoch 2] Validation indivisual [0] class recall: 0.241847\n",
      "2018-12-03 22:35:07.367671  [Epoch 2] Validation indivisual [0] class precision: 0.977116\n",
      "2018-12-03 22:35:07.369807  [Epoch 2] Validation indivisual [1] class recall: 0.249535\n",
      "2018-12-03 22:35:07.371730  [Epoch 2] Validation indivisual [1] class precision: 0.991289\n",
      "2018-12-03 22:35:07.373580  [Epoch 2] Validation indivisual [2] class recall: 0.227140\n",
      "2018-12-03 22:35:07.375301  [Epoch 2] Validation indivisual [2] class precision: 0.902019\n",
      "2018-12-03 22:35:07.377198  \n",
      "\n",
      " Train one epoch   4 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:44:45.692463  [Epoch 3] mean loss: 0.002123\n",
      "2018-12-03 22:44:45.694427  [Epoch 3] heading rmse[deg]: 104.489407\n",
      "2018-12-03 22:44:45.696231  [Epoch 3] class accuracy: 0.965746\n",
      "2018-12-03 22:44:45.698053  [Epoch 3] avg class acc: 0.965491\n",
      "2018-12-03 22:44:45.699950  [Epoch 3] indivisual [0] class recall: 0.955766\n",
      "2018-12-03 22:44:45.701864  [Epoch 3] indivisual [0] class precision: 0.966241\n",
      "2018-12-03 22:44:45.703697  [Epoch 3] indivisual [1] class recall: 0.975715\n",
      "2018-12-03 22:44:45.705415  [Epoch 3] indivisual [1] class precision: 0.980368\n",
      "2018-12-03 22:44:45.707053  [Epoch 3] indivisual [2] class recall: 0.964992\n",
      "2018-12-03 22:44:45.708722  [Epoch 3] indivisual [2] class precision: 0.908739\n",
      "2018-12-03 22:44:45.710300   Evaluation one (validation set) epoch   4 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:45:31.185292  [Epoch 3] Validation mean loss: 0.002456\n",
      "2018-12-03 22:45:31.187122  [Epoch 3] Validation heading rmse[deg]: 105.155307\n",
      "2018-12-03 22:45:31.188912  [Epoch 3] Validation class accuracy: 0.977249\n",
      "2018-12-03 22:45:31.190675  [Epoch 3] Validation avg class acc: 0.238943\n",
      "2018-12-03 22:45:31.192336  [Epoch 3] Validation indivisual [0] class recall: 0.245377\n",
      "2018-12-03 22:45:31.194057  [Epoch 3] Validation indivisual [0] class precision: 0.972283\n",
      "2018-12-03 22:45:31.195944  [Epoch 3] Validation indivisual [1] class recall: 0.248257\n",
      "2018-12-03 22:45:31.197899  [Epoch 3] Validation indivisual [1] class precision: 0.989936\n",
      "2018-12-03 22:45:31.199712  [Epoch 3] Validation indivisual [2] class recall: 0.223195\n",
      "2018-12-03 22:45:31.201538  [Epoch 3] Validation indivisual [2] class precision: 0.943935\n",
      "2018-12-03 22:45:31.203285  \n",
      "\n",
      " Train one epoch   5 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:55:09.314881  [Epoch 4] mean loss: 0.001756\n",
      "2018-12-03 22:55:09.316976  [Epoch 4] heading rmse[deg]: 104.408343\n",
      "2018-12-03 22:55:09.321850  [Epoch 4] class accuracy: 0.972000\n",
      "2018-12-03 22:55:09.324872  [Epoch 4] avg class acc: 0.970998\n",
      "2018-12-03 22:55:09.326815  [Epoch 4] indivisual [0] class recall: 0.964042\n",
      "2018-12-03 22:55:09.328581  [Epoch 4] indivisual [0] class precision: 0.972228\n",
      "2018-12-03 22:55:09.330177  [Epoch 4] indivisual [1] class recall: 0.980739\n",
      "2018-12-03 22:55:09.331895  [Epoch 4] indivisual [1] class precision: 0.983018\n",
      "2018-12-03 22:55:09.333657  [Epoch 4] indivisual [2] class recall: 0.968212\n",
      "2018-12-03 22:55:09.335653  [Epoch 4] indivisual [2] class precision: 0.928678\n",
      "2018-12-03 22:55:09.337327   Evaluation one (validation set) epoch   5 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 22:55:54.912946  [Epoch 4] Validation mean loss: 0.002240\n",
      "2018-12-03 22:55:54.915069  [Epoch 4] Validation heading rmse[deg]: 103.840658\n",
      "2018-12-03 22:55:54.917004  [Epoch 4] Validation class accuracy: 0.973271\n",
      "2018-12-03 22:55:54.919008  [Epoch 4] Validation avg class acc: 0.239346\n",
      "2018-12-03 22:55:54.920713  [Epoch 4] Validation indivisual [0] class recall: 0.244182\n",
      "2018-12-03 22:55:54.922577  [Epoch 4] Validation indivisual [0] class precision: 0.965970\n",
      "2018-12-03 22:55:54.924377  [Epoch 4] Validation indivisual [1] class recall: 0.246075\n",
      "2018-12-03 22:55:54.926120  [Epoch 4] Validation indivisual [1] class precision: 0.993818\n",
      "2018-12-03 22:55:54.927958  [Epoch 4] Validation indivisual [2] class recall: 0.227781\n",
      "2018-12-03 22:55:54.929842  [Epoch 4] Validation indivisual [2] class precision: 0.919865\n",
      "2018-12-03 22:55:54.931537  \n",
      "\n",
      " Train one epoch   6 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:05:33.470678  [Epoch 5] mean loss: 0.001017\n",
      "2018-12-03 23:05:33.472719  [Epoch 5] heading rmse[deg]: 104.066295\n",
      "2018-12-03 23:05:33.474684  [Epoch 5] class accuracy: 0.985437\n",
      "2018-12-03 23:05:33.476771  [Epoch 5] avg class acc: 0.983932\n",
      "2018-12-03 23:05:33.478573  [Epoch 5] indivisual [0] class recall: 0.980911\n",
      "2018-12-03 23:05:33.480476  [Epoch 5] indivisual [0] class precision: 0.986130\n",
      "2018-12-03 23:05:33.482217  [Epoch 5] indivisual [1] class recall: 0.991334\n",
      "2018-12-03 23:05:33.483880  [Epoch 5] indivisual [1] class precision: 0.992485\n",
      "2018-12-03 23:05:33.485423  [Epoch 5] indivisual [2] class recall: 0.979551\n",
      "2018-12-03 23:05:33.486909  [Epoch 5] indivisual [2] class precision: 0.955137\n",
      "2018-12-03 23:05:33.488514   Evaluation one (validation set) epoch   6 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:06:18.870800  [Epoch 5] Validation mean loss: 0.002096\n",
      "2018-12-03 23:06:18.872661  [Epoch 5] Validation heading rmse[deg]: 103.820206\n",
      "2018-12-03 23:06:18.874432  [Epoch 5] Validation class accuracy: 0.984572\n",
      "2018-12-03 23:06:18.876406  [Epoch 5] Validation avg class acc: 0.240993\n",
      "2018-12-03 23:06:18.878070  [Epoch 5] Validation indivisual [0] class recall: 0.247612\n",
      "2018-12-03 23:06:18.879765  [Epoch 5] Validation indivisual [0] class precision: 0.978782\n",
      "2018-12-03 23:06:18.881592  [Epoch 5] Validation indivisual [1] class recall: 0.249478\n",
      "2018-12-03 23:06:18.883521  [Epoch 5] Validation indivisual [1] class precision: 0.993491\n",
      "2018-12-03 23:06:18.885260  [Epoch 5] Validation indivisual [2] class recall: 0.225890\n",
      "2018-12-03 23:06:18.886961  [Epoch 5] Validation indivisual [2] class precision: 0.970630\n",
      "2018-12-03 23:06:18.888617  \n",
      "\n",
      " Train one epoch   7 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:15:58.235987  [Epoch 6] mean loss: 0.000870\n",
      "2018-12-03 23:15:58.238029  [Epoch 6] heading rmse[deg]: 104.152940\n",
      "2018-12-03 23:15:58.239956  [Epoch 6] class accuracy: 0.987616\n",
      "2018-12-03 23:15:58.241818  [Epoch 6] avg class acc: 0.985465\n",
      "2018-12-03 23:15:58.243604  [Epoch 6] indivisual [0] class recall: 0.984598\n",
      "2018-12-03 23:15:58.245474  [Epoch 6] indivisual [0] class precision: 0.987267\n",
      "2018-12-03 23:15:58.247112  [Epoch 6] indivisual [1] class recall: 0.992687\n",
      "2018-12-03 23:15:58.248683  [Epoch 6] indivisual [1] class precision: 0.993537\n",
      "2018-12-03 23:15:58.250469  [Epoch 6] indivisual [2] class recall: 0.979109\n",
      "2018-12-03 23:15:58.252203  [Epoch 6] indivisual [2] class precision: 0.965483\n",
      "2018-12-03 23:15:58.254015   Evaluation one (validation set) epoch   7 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:16:43.785680  [Epoch 6] Validation mean loss: 0.002108\n",
      "2018-12-03 23:16:43.787525  [Epoch 6] Validation heading rmse[deg]: 103.932513\n",
      "2018-12-03 23:16:43.789331  [Epoch 6] Validation class accuracy: 0.983717\n",
      "2018-12-03 23:16:43.791189  [Epoch 6] Validation avg class acc: 0.241486\n",
      "2018-12-03 23:16:43.792905  [Epoch 6] Validation indivisual [0] class recall: 0.245900\n",
      "2018-12-03 23:16:43.794573  [Epoch 6] Validation indivisual [0] class precision: 0.984350\n",
      "2018-12-03 23:16:43.796420  [Epoch 6] Validation indivisual [1] class recall: 0.250021\n",
      "2018-12-03 23:16:43.798088  [Epoch 6] Validation indivisual [1] class precision: 0.988224\n",
      "2018-12-03 23:16:43.799803  [Epoch 6] Validation indivisual [2] class recall: 0.228538\n",
      "2018-12-03 23:16:43.801458  [Epoch 6] Validation indivisual [2] class precision: 0.961768\n",
      "2018-12-03 23:16:43.803020  \n",
      "\n",
      " Train one epoch   8 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:26:22.509080  [Epoch 7] mean loss: 0.000669\n",
      "2018-12-03 23:26:22.511177  [Epoch 7] heading rmse[deg]: 103.995613\n",
      "2018-12-03 23:26:22.513434  [Epoch 7] class accuracy: 0.991418\n",
      "2018-12-03 23:26:22.515488  [Epoch 7] avg class acc: 0.989909\n",
      "2018-12-03 23:26:22.517224  [Epoch 7] indivisual [0] class recall: 0.989694\n",
      "2018-12-03 23:26:22.519506  [Epoch 7] indivisual [0] class precision: 0.990810\n",
      "2018-12-03 23:26:22.521437  [Epoch 7] indivisual [1] class recall: 0.994591\n",
      "2018-12-03 23:26:22.523182  [Epoch 7] indivisual [1] class precision: 0.995580\n",
      "2018-12-03 23:26:22.524843  [Epoch 7] indivisual [2] class recall: 0.985443\n",
      "2018-12-03 23:26:22.526500  [Epoch 7] indivisual [2] class precision: 0.977190\n",
      "2018-12-03 23:26:22.528213   Evaluation one (validation set) epoch   8 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:27:08.015150  [Epoch 7] Validation mean loss: 0.001972\n",
      "2018-12-03 23:27:08.017175  [Epoch 7] Validation heading rmse[deg]: 104.665479\n",
      "2018-12-03 23:27:08.018922  [Epoch 7] Validation class accuracy: 0.986245\n",
      "2018-12-03 23:27:08.020825  [Epoch 7] Validation avg class acc: 0.241340\n",
      "2018-12-03 23:27:08.022499  [Epoch 7] Validation indivisual [0] class recall: 0.247929\n",
      "2018-12-03 23:27:08.024414  [Epoch 7] Validation indivisual [0] class precision: 0.982414\n",
      "2018-12-03 23:27:08.026355  [Epoch 7] Validation indivisual [1] class recall: 0.250062\n",
      "2018-12-03 23:27:08.028209  [Epoch 7] Validation indivisual [1] class precision: 0.991714\n",
      "2018-12-03 23:27:08.029905  [Epoch 7] Validation indivisual [2] class recall: 0.226030\n",
      "2018-12-03 23:27:08.031591  [Epoch 7] Validation indivisual [2] class precision: 0.978700\n",
      "2018-12-03 23:27:08.033516  \n",
      "\n",
      " Train one epoch   9 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:36:46.413469  [Epoch 8] mean loss: 0.000498\n",
      "2018-12-03 23:36:46.415376  [Epoch 8] heading rmse[deg]: 103.956070\n",
      "2018-12-03 23:36:46.417280  [Epoch 8] class accuracy: 0.995059\n",
      "2018-12-03 23:36:46.419332  [Epoch 8] avg class acc: 0.993925\n",
      "2018-12-03 23:36:46.421471  [Epoch 8] indivisual [0] class recall: 0.994228\n",
      "2018-12-03 23:36:46.423319  [Epoch 8] indivisual [0] class precision: 0.994536\n",
      "2018-12-03 23:36:46.425099  [Epoch 8] indivisual [1] class recall: 0.996992\n",
      "2018-12-03 23:36:46.426775  [Epoch 8] indivisual [1] class precision: 0.997350\n",
      "2018-12-03 23:36:46.428536  [Epoch 8] indivisual [2] class recall: 0.990556\n",
      "2018-12-03 23:36:46.430168  [Epoch 8] indivisual [2] class precision: 0.987921\n",
      "2018-12-03 23:36:46.431858   Evaluation one (validation set) epoch   9 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:37:31.900783  [Epoch 8] Validation mean loss: 0.002431\n",
      "2018-12-03 23:37:31.902780  [Epoch 8] Validation heading rmse[deg]: 103.805798\n",
      "2018-12-03 23:37:31.904448  [Epoch 8] Validation class accuracy: 0.984498\n",
      "2018-12-03 23:37:31.906203  [Epoch 8] Validation avg class acc: 0.241123\n",
      "2018-12-03 23:37:31.907949  [Epoch 8] Validation indivisual [0] class recall: 0.246544\n",
      "2018-12-03 23:37:31.909565  [Epoch 8] Validation indivisual [0] class precision: 0.985636\n",
      "2018-12-03 23:37:31.911329  [Epoch 8] Validation indivisual [1] class recall: 0.250331\n",
      "2018-12-03 23:37:31.913233  [Epoch 8] Validation indivisual [1] class precision: 0.987031\n",
      "2018-12-03 23:37:31.914919  [Epoch 8] Validation indivisual [2] class recall: 0.226492\n",
      "2018-12-03 23:37:31.916679  [Epoch 8] Validation indivisual [2] class precision: 0.968627\n",
      "2018-12-03 23:37:31.918367  \n",
      "\n",
      " Train one epoch  10 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:47:10.841935  [Epoch 9] mean loss: 0.000468\n",
      "2018-12-03 23:47:10.843955  [Epoch 9] heading rmse[deg]: 103.959266\n",
      "2018-12-03 23:47:10.845780  [Epoch 9] class accuracy: 0.995282\n",
      "2018-12-03 23:47:10.847814  [Epoch 9] avg class acc: 0.994206\n",
      "2018-12-03 23:47:10.850076  [Epoch 9] indivisual [0] class recall: 0.994847\n",
      "2018-12-03 23:47:10.851915  [Epoch 9] indivisual [0] class precision: 0.994455\n",
      "2018-12-03 23:47:10.853715  [Epoch 9] indivisual [1] class recall: 0.996771\n",
      "2018-12-03 23:47:10.855428  [Epoch 9] indivisual [1] class precision: 0.997459\n",
      "2018-12-03 23:47:10.857287  [Epoch 9] indivisual [2] class recall: 0.990999\n",
      "2018-12-03 23:47:10.859025  [Epoch 9] indivisual [2] class precision: 0.989789\n",
      "2018-12-03 23:47:10.860810   Evaluation one (validation set) epoch  10 /  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-03 23:47:56.281624  [Epoch 9] Validation mean loss: 0.002104\n",
      "2018-12-03 23:47:56.283510  [Epoch 9] Validation heading rmse[deg]: 103.889328\n",
      "2018-12-03 23:47:56.285236  [Epoch 9] Validation class accuracy: 0.986952\n",
      "2018-12-03 23:47:56.287070  [Epoch 9] Validation avg class acc: 0.242281\n",
      "2018-12-03 23:47:56.288806  [Epoch 9] Validation indivisual [0] class recall: 0.247210\n",
      "2018-12-03 23:47:56.290429  [Epoch 9] Validation indivisual [0] class precision: 0.987005\n",
      "2018-12-03 23:47:56.293426  [Epoch 9] Validation indivisual [1] class recall: 0.250352\n",
      "2018-12-03 23:47:56.295318  [Epoch 9] Validation indivisual [1] class precision: 0.990830\n",
      "2018-12-03 23:47:56.297235  [Epoch 9] Validation indivisual [2] class recall: 0.229281\n",
      "2018-12-03 23:47:56.298992  [Epoch 9] Validation indivisual [2] class precision: 0.970018\n",
      "2018-12-03 23:47:57.212019  Model saved in file: /home/gozilla/kyungpyo/git/MasterThesisProject/Network/notebook/train_log_20181203/model_out5/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "    log_path = os.path.abspath(os.path.join(model_save_path, 'train_log.txt') )\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(200000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 10\n",
    "\n",
    "    \"\"\" log file \"\"\"\n",
    "    LOG_FOUT = open(log_path, 'w')\n",
    "\n",
    "    def log_string(out_str):\n",
    "        out_str = str( datetime.datetime.fromtimestamp( time.time() ) ) + \"  \" + out_str\n",
    "        LOG_FOUT.write(out_str+'\\n')\n",
    "        LOG_FOUT.flush()\n",
    "        print(out_str)\n",
    "\n",
    "\n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" load traing model \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            log_string ( \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "\n",
    "            log_string(  '[Epoch %d] mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            log_string ( \" Evaluation one (validation set) epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = False\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_true_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "                \n",
    "                \n",
    "                \n",
    "            log_string(  '[Epoch %d] Validation mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] Validation class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_3:0\", shape=(), dtype=bool, device=/device:GPU:0)\n",
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n",
      "2018-12-03 23:56:46.939556  \n",
      "\n",
      " Train one epoch   1 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:43<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:06:31.117574  [Epoch 0] mean loss: 0.012465\n",
      "2018-12-04 00:06:31.119302  [Epoch 0] heading rmse[deg]: 149.063311\n",
      "2018-12-04 00:06:31.120755  [Epoch 0] class accuracy: 0.747480\n",
      "2018-12-04 00:06:31.122302  [Epoch 0] avg class acc: 0.775305\n",
      "2018-12-04 00:06:31.123772  [Epoch 0] indivisual [0] class recall: 0.675781\n",
      "2018-12-04 00:06:31.125255  [Epoch 0] indivisual [0] class precision: 0.736233\n",
      "2018-12-04 00:06:31.126639  [Epoch 0] indivisual [1] class recall: 0.789690\n",
      "2018-12-04 00:06:31.128123  [Epoch 0] indivisual [1] class precision: 0.738344\n",
      "2018-12-04 00:06:31.129521  [Epoch 0] indivisual [2] class recall: 0.860444\n",
      "2018-12-04 00:06:31.130892  [Epoch 0] indivisual [2] class precision: 0.824181\n",
      "2018-12-04 00:06:31.132305   Evaluation one (validation set) epoch   1 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:07:16.993339  [Epoch 0] Validation mean loss: 0.004854\n",
      "2018-12-04 00:07:16.995367  [Epoch 0] Validation heading rmse[deg]: 105.124950\n",
      "2018-12-04 00:07:16.997357  [Epoch 0] Validation class accuracy: 0.931896\n",
      "2018-12-04 00:07:16.999212  [Epoch 0] Validation avg class acc: 0.231221\n",
      "2018-12-04 00:07:17.001068  [Epoch 0] Validation indivisual [0] class recall: 0.240231\n",
      "2018-12-04 00:07:17.002867  [Epoch 0] Validation indivisual [0] class precision: 0.892369\n",
      "2018-12-04 00:07:17.004587  [Epoch 0] Validation indivisual [1] class recall: 0.227337\n",
      "2018-12-04 00:07:17.006207  [Epoch 0] Validation indivisual [1] class precision: 0.986003\n",
      "2018-12-04 00:07:17.007888  [Epoch 0] Validation indivisual [2] class recall: 0.226094\n",
      "2018-12-04 00:07:17.009618  [Epoch 0] Validation indivisual [2] class precision: 0.898906\n",
      "2018-12-04 00:07:17.011299  \n",
      "\n",
      " Train one epoch   2 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:40<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:16:58.274600  [Epoch 1] mean loss: 0.005925\n",
      "2018-12-04 00:16:58.276807  [Epoch 1] heading rmse[deg]: 105.150640\n",
      "2018-12-04 00:16:58.278669  [Epoch 1] class accuracy: 0.891802\n",
      "2018-12-04 00:16:58.280594  [Epoch 1] avg class acc: 0.900821\n",
      "2018-12-04 00:16:58.282365  [Epoch 1] indivisual [0] class recall: 0.863132\n",
      "2018-12-04 00:16:58.284153  [Epoch 1] indivisual [0] class precision: 0.888576\n",
      "2018-12-04 00:16:58.285889  [Epoch 1] indivisual [1] class recall: 0.910774\n",
      "2018-12-04 00:16:58.287616  [Epoch 1] indivisual [1] class precision: 0.899651\n",
      "2018-12-04 00:16:58.289237  [Epoch 1] indivisual [2] class recall: 0.928556\n",
      "2018-12-04 00:16:58.290732  [Epoch 1] indivisual [2] class precision: 0.873341\n",
      "2018-12-04 00:16:58.292319   Evaluation one (validation set) epoch   2 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:17:43.993407  [Epoch 1] Validation mean loss: 0.003100\n",
      "2018-12-04 00:17:43.995350  [Epoch 1] Validation heading rmse[deg]: 104.091046\n",
      "2018-12-04 00:17:43.997194  [Epoch 1] Validation class accuracy: 0.965353\n",
      "2018-12-04 00:17:43.998911  [Epoch 1] Validation avg class acc: 0.237199\n",
      "2018-12-04 00:17:44.000461  [Epoch 1] Validation indivisual [0] class recall: 0.242392\n",
      "2018-12-04 00:17:44.002066  [Epoch 1] Validation indivisual [0] class precision: 0.953129\n",
      "2018-12-04 00:17:44.003632  [Epoch 1] Validation indivisual [1] class recall: 0.244083\n",
      "2018-12-04 00:17:44.005406  [Epoch 1] Validation indivisual [1] class precision: 0.990929\n",
      "2018-12-04 00:17:44.007344  [Epoch 1] Validation indivisual [2] class recall: 0.225121\n",
      "2018-12-04 00:17:44.009142  [Epoch 1] Validation indivisual [2] class precision: 0.912192\n",
      "2018-12-04 00:17:44.010914  \n",
      "\n",
      " Train one epoch   3 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:40<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:27:25.427066  [Epoch 2] mean loss: 0.003915\n",
      "2018-12-04 00:27:25.428859  [Epoch 2] heading rmse[deg]: 104.526460\n",
      "2018-12-04 00:27:25.430638  [Epoch 2] class accuracy: 0.931269\n",
      "2018-12-04 00:27:25.432513  [Epoch 2] avg class acc: 0.934841\n",
      "2018-12-04 00:27:25.434386  [Epoch 2] indivisual [0] class recall: 0.909106\n",
      "2018-12-04 00:27:25.436253  [Epoch 2] indivisual [0] class precision: 0.933526\n",
      "2018-12-04 00:27:25.438104  [Epoch 2] indivisual [1] class recall: 0.949305\n",
      "2018-12-04 00:27:25.440038  [Epoch 2] indivisual [1] class precision: 0.940043\n",
      "2018-12-04 00:27:25.442023  [Epoch 2] indivisual [2] class recall: 0.946111\n",
      "2018-12-04 00:27:25.443711  [Epoch 2] indivisual [2] class precision: 0.889574\n",
      "2018-12-04 00:27:25.445488   Evaluation one (validation set) epoch   3 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:28:11.113080  [Epoch 2] Validation mean loss: 0.002403\n",
      "2018-12-04 00:28:11.114960  [Epoch 2] Validation heading rmse[deg]: 103.826852\n",
      "2018-12-04 00:28:11.116749  [Epoch 2] Validation class accuracy: 0.977621\n",
      "2018-12-04 00:28:11.118506  [Epoch 2] Validation avg class acc: 0.240651\n",
      "2018-12-04 00:28:11.120398  [Epoch 2] Validation indivisual [0] class recall: 0.242693\n",
      "2018-12-04 00:28:11.122330  [Epoch 2] Validation indivisual [0] class precision: 0.985073\n",
      "2018-12-04 00:28:11.124166  [Epoch 2] Validation indivisual [1] class recall: 0.249452\n",
      "2018-12-04 00:28:11.125926  [Epoch 2] Validation indivisual [1] class precision: 0.986905\n",
      "2018-12-04 00:28:11.127709  [Epoch 2] Validation indivisual [2] class recall: 0.229807\n",
      "2018-12-04 00:28:11.129459  [Epoch 2] Validation indivisual [2] class precision: 0.911405\n",
      "2018-12-04 00:28:11.131117  \n",
      "\n",
      " Train one epoch   4 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:37:50.931524  [Epoch 3] mean loss: 0.002700\n",
      "2018-12-04 00:37:50.933517  [Epoch 3] heading rmse[deg]: 104.465866\n",
      "2018-12-04 00:37:50.935271  [Epoch 3] class accuracy: 0.957536\n",
      "2018-12-04 00:37:50.937199  [Epoch 3] avg class acc: 0.959125\n",
      "2018-12-04 00:37:50.939228  [Epoch 3] indivisual [0] class recall: 0.944870\n",
      "2018-12-04 00:37:50.941173  [Epoch 3] indivisual [0] class precision: 0.958335\n",
      "2018-12-04 00:37:50.942721  [Epoch 3] indivisual [1] class recall: 0.968291\n",
      "2018-12-04 00:37:50.944494  [Epoch 3] indivisual [1] class precision: 0.968425\n",
      "2018-12-04 00:37:50.946181  [Epoch 3] indivisual [2] class recall: 0.964214\n",
      "2018-12-04 00:37:50.947955  [Epoch 3] indivisual [2] class precision: 0.913071\n",
      "2018-12-04 00:37:50.949630   Evaluation one (validation set) epoch   4 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:38:36.569614  [Epoch 3] Validation mean loss: 0.002749\n",
      "2018-12-04 00:38:36.571307  [Epoch 3] Validation heading rmse[deg]: 104.699792\n",
      "2018-12-04 00:38:36.572986  [Epoch 3] Validation class accuracy: 0.974349\n",
      "2018-12-04 00:38:36.574698  [Epoch 3] Validation avg class acc: 0.237256\n",
      "2018-12-04 00:38:36.576546  [Epoch 3] Validation indivisual [0] class recall: 0.245441\n",
      "2018-12-04 00:38:36.578231  [Epoch 3] Validation indivisual [0] class precision: 0.965179\n",
      "2018-12-04 00:38:36.580124  [Epoch 3] Validation indivisual [1] class recall: 0.247719\n",
      "2018-12-04 00:38:36.582007  [Epoch 3] Validation indivisual [1] class precision: 0.990569\n",
      "2018-12-04 00:38:36.583728  [Epoch 3] Validation indivisual [2] class recall: 0.218609\n",
      "2018-12-04 00:38:36.585472  [Epoch 3] Validation indivisual [2] class precision: 0.943505\n",
      "2018-12-04 00:38:36.587160  \n",
      "\n",
      " Train one epoch   5 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:40<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:48:18.381281  [Epoch 4] mean loss: 0.002007\n",
      "2018-12-04 00:48:18.383117  [Epoch 4] heading rmse[deg]: 104.416842\n",
      "2018-12-04 00:48:18.386053  [Epoch 4] class accuracy: 0.970613\n",
      "2018-12-04 00:48:18.387859  [Epoch 4] avg class acc: 0.970151\n",
      "2018-12-04 00:48:18.389403  [Epoch 4] indivisual [0] class recall: 0.962297\n",
      "2018-12-04 00:48:18.390926  [Epoch 4] indivisual [0] class precision: 0.970853\n",
      "2018-12-04 00:48:18.392437  [Epoch 4] indivisual [1] class recall: 0.979166\n",
      "2018-12-04 00:48:18.393902  [Epoch 4] indivisual [1] class precision: 0.981658\n",
      "2018-12-04 00:48:18.395499  [Epoch 4] indivisual [2] class recall: 0.968990\n",
      "2018-12-04 00:48:18.397003  [Epoch 4] indivisual [2] class precision: 0.927250\n",
      "2018-12-04 00:48:18.398516   Evaluation one (validation set) epoch   5 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:49:03.973772  [Epoch 4] Validation mean loss: 0.002380\n",
      "2018-12-04 00:49:03.975535  [Epoch 4] Validation heading rmse[deg]: 103.880617\n",
      "2018-12-04 00:49:03.977400  [Epoch 4] Validation class accuracy: 0.977435\n",
      "2018-12-04 00:49:03.979158  [Epoch 4] Validation avg class acc: 0.239311\n",
      "2018-12-04 00:49:03.980842  [Epoch 4] Validation indivisual [0] class recall: 0.245345\n",
      "2018-12-04 00:49:03.982667  [Epoch 4] Validation indivisual [0] class precision: 0.975216\n",
      "2018-12-04 00:49:03.984709  [Epoch 4] Validation indivisual [1] class recall: 0.248061\n",
      "2018-12-04 00:49:03.986448  [Epoch 4] Validation indivisual [1] class precision: 0.990092\n",
      "2018-12-04 00:49:03.988240  [Epoch 4] Validation indivisual [2] class recall: 0.224529\n",
      "2018-12-04 00:49:03.989937  [Epoch 4] Validation indivisual [2] class precision: 0.933449\n",
      "2018-12-04 00:49:03.991713  \n",
      "\n",
      " Train one epoch   6 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:41<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:58:46.171988  [Epoch 5] mean loss: 0.001280\n",
      "2018-12-04 00:58:46.173730  [Epoch 5] heading rmse[deg]: 104.056772\n",
      "2018-12-04 00:58:46.175369  [Epoch 5] class accuracy: 0.983368\n",
      "2018-12-04 00:58:46.177043  [Epoch 5] avg class acc: 0.982020\n",
      "2018-12-04 00:58:46.178584  [Epoch 5] indivisual [0] class recall: 0.978827\n",
      "2018-12-04 00:58:46.180170  [Epoch 5] indivisual [0] class precision: 0.983340\n",
      "2018-12-04 00:58:46.182086  [Epoch 5] indivisual [1] class recall: 0.989127\n",
      "2018-12-04 00:58:46.184128  [Epoch 5] indivisual [1] class precision: 0.990411\n",
      "2018-12-04 00:58:46.185981  [Epoch 5] indivisual [2] class recall: 0.978106\n",
      "2018-12-04 00:58:46.187802  [Epoch 5] indivisual [2] class precision: 0.955799\n",
      "2018-12-04 00:58:46.189499   Evaluation one (validation set) epoch   6 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 00:59:31.656718  [Epoch 5] Validation mean loss: 0.002491\n",
      "2018-12-04 00:59:31.658549  [Epoch 5] Validation heading rmse[deg]: 103.800788\n",
      "2018-12-04 00:59:31.660255  [Epoch 5] Validation class accuracy: 0.978959\n",
      "2018-12-04 00:59:31.661908  [Epoch 5] Validation avg class acc: 0.239895\n",
      "2018-12-04 00:59:31.663737  [Epoch 5] Validation indivisual [0] class recall: 0.245541\n",
      "2018-12-04 00:59:31.665669  [Epoch 5] Validation indivisual [0] class precision: 0.972220\n",
      "2018-12-04 00:59:31.667515  [Epoch 5] Validation indivisual [1] class recall: 0.248422\n",
      "2018-12-04 00:59:31.669249  [Epoch 5] Validation indivisual [1] class precision: 0.995605\n",
      "2018-12-04 00:59:31.670856  [Epoch 5] Validation indivisual [2] class recall: 0.225723\n",
      "2018-12-04 00:59:31.672537  [Epoch 5] Validation indivisual [2] class precision: 0.937349\n",
      "2018-12-04 00:59:31.674158  \n",
      "\n",
      " Train one epoch   7 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:09:11.003495  [Epoch 6] mean loss: 0.001149\n",
      "2018-12-04 01:09:11.005337  [Epoch 6] heading rmse[deg]: 104.165578\n",
      "2018-12-04 01:09:11.007081  [Epoch 6] class accuracy: 0.986192\n",
      "2018-12-04 01:09:11.009053  [Epoch 6] avg class acc: 0.984956\n",
      "2018-12-04 01:09:11.010713  [Epoch 6] indivisual [0] class recall: 0.983444\n",
      "2018-12-04 01:09:11.012356  [Epoch 6] indivisual [0] class precision: 0.985247\n",
      "2018-12-04 01:09:11.013973  [Epoch 6] indivisual [1] class recall: 0.990093\n",
      "2018-12-04 01:09:11.015680  [Epoch 6] indivisual [1] class precision: 0.991982\n",
      "2018-12-04 01:09:11.017278  [Epoch 6] indivisual [2] class recall: 0.981331\n",
      "2018-12-04 01:09:11.018822  [Epoch 6] indivisual [2] class precision: 0.966933\n",
      "2018-12-04 01:09:11.020425   Evaluation one (validation set) epoch   7 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:09:56.544599  [Epoch 6] Validation mean loss: 0.002540\n",
      "2018-12-04 01:09:56.546477  [Epoch 6] Validation heading rmse[deg]: 104.004489\n",
      "2018-12-04 01:09:56.548383  [Epoch 6] Validation class accuracy: 0.979888\n",
      "2018-12-04 01:09:56.550301  [Epoch 6] Validation avg class acc: 0.240301\n",
      "2018-12-04 01:09:56.552149  [Epoch 6] Validation indivisual [0] class recall: 0.246640\n",
      "2018-12-04 01:09:56.554085  [Epoch 6] Validation indivisual [0] class precision: 0.973638\n",
      "2018-12-04 01:09:56.556014  [Epoch 6] Validation indivisual [1] class recall: 0.247641\n",
      "2018-12-04 01:09:56.557755  [Epoch 6] Validation indivisual [1] class precision: 0.991550\n",
      "2018-12-04 01:09:56.559554  [Epoch 6] Validation indivisual [2] class recall: 0.226621\n",
      "2018-12-04 01:09:56.561240  [Epoch 6] Validation indivisual [2] class precision: 0.956721\n",
      "2018-12-04 01:09:56.562857  \n",
      "\n",
      " Train one epoch   8 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:19:35.314929  [Epoch 7] mean loss: 0.000862\n",
      "2018-12-04 01:19:35.316915  [Epoch 7] heading rmse[deg]: 103.988421\n",
      "2018-12-04 01:19:35.318637  [Epoch 7] class accuracy: 0.990861\n",
      "2018-12-04 01:19:35.320516  [Epoch 7] avg class acc: 0.988907\n",
      "2018-12-04 01:19:35.322269  [Epoch 7] indivisual [0] class recall: 0.989160\n",
      "2018-12-04 01:19:35.324250  [Epoch 7] indivisual [0] class precision: 0.990079\n",
      "2018-12-04 01:19:35.326244  [Epoch 7] indivisual [1] class recall: 0.994453\n",
      "2018-12-04 01:19:35.327990  [Epoch 7] indivisual [1] class precision: 0.995002\n",
      "2018-12-04 01:19:35.329722  [Epoch 7] indivisual [2] class recall: 0.983109\n",
      "2018-12-04 01:19:35.331343  [Epoch 7] indivisual [2] class precision: 0.977353\n",
      "2018-12-04 01:19:35.333053   Evaluation one (validation set) epoch   8 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:20:20.739774  [Epoch 7] Validation mean loss: 0.001946\n",
      "2018-12-04 01:20:20.741732  [Epoch 7] Validation heading rmse[deg]: 104.597921\n",
      "2018-12-04 01:20:20.743524  [Epoch 7] Validation class accuracy: 0.987546\n",
      "2018-12-04 01:20:20.745256  [Epoch 7] Validation avg class acc: 0.242958\n",
      "2018-12-04 01:20:20.747089  [Epoch 7] Validation indivisual [0] class recall: 0.247527\n",
      "2018-12-04 01:20:20.749119  [Epoch 7] Validation indivisual [0] class precision: 0.987771\n",
      "2018-12-04 01:20:20.750846  [Epoch 7] Validation indivisual [1] class recall: 0.249814\n",
      "2018-12-04 01:20:20.752603  [Epoch 7] Validation indivisual [1] class precision: 0.990892\n",
      "2018-12-04 01:20:20.754233  [Epoch 7] Validation indivisual [2] class recall: 0.231532\n",
      "2018-12-04 01:20:20.755931  [Epoch 7] Validation indivisual [2] class precision: 0.972339\n",
      "2018-12-04 01:20:20.757584  \n",
      "\n",
      " Train one epoch   9 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:29:59.323729  [Epoch 8] mean loss: 0.000857\n",
      "2018-12-04 01:29:59.325603  [Epoch 8] heading rmse[deg]: 103.949283\n",
      "2018-12-04 01:29:59.327391  [Epoch 8] class accuracy: 0.991641\n",
      "2018-12-04 01:29:59.329268  [Epoch 8] avg class acc: 0.990581\n",
      "2018-12-04 01:29:59.330938  [Epoch 8] indivisual [0] class recall: 0.990426\n",
      "2018-12-04 01:29:59.332646  [Epoch 8] indivisual [0] class precision: 0.990677\n",
      "2018-12-04 01:29:59.334604  [Epoch 8] indivisual [1] class recall: 0.993873\n",
      "2018-12-04 01:29:59.336682  [Epoch 8] indivisual [1] class precision: 0.994615\n",
      "2018-12-04 01:29:59.338419  [Epoch 8] indivisual [2] class recall: 0.987444\n",
      "2018-12-04 01:29:59.340137  [Epoch 8] indivisual [2] class precision: 0.983510\n",
      "2018-12-04 01:29:59.341819   Evaluation one (validation set) epoch   9 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:30:44.847495  [Epoch 8] Validation mean loss: 0.002146\n",
      "2018-12-04 01:30:44.849403  [Epoch 8] Validation heading rmse[deg]: 103.879990\n",
      "2018-12-04 01:30:44.851062  [Epoch 8] Validation class accuracy: 0.986022\n",
      "2018-12-04 01:30:44.852915  [Epoch 8] Validation avg class acc: 0.242516\n",
      "2018-12-04 01:30:44.854584  [Epoch 8] Validation indivisual [0] class recall: 0.247052\n",
      "2018-12-04 01:30:44.856317  [Epoch 8] Validation indivisual [0] class precision: 0.986497\n",
      "2018-12-04 01:30:44.857954  [Epoch 8] Validation indivisual [1] class recall: 0.249586\n",
      "2018-12-04 01:30:44.859609  [Epoch 8] Validation indivisual [1] class precision: 0.991209\n",
      "2018-12-04 01:30:44.861254  [Epoch 8] Validation indivisual [2] class recall: 0.230910\n",
      "2018-12-04 01:30:44.862817  [Epoch 8] Validation indivisual [2] class precision: 0.962140\n",
      "2018-12-04 01:30:44.864603  \n",
      "\n",
      " Train one epoch  10 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:40:23.600299  [Epoch 9] mean loss: 0.000693\n",
      "2018-12-04 01:40:23.602249  [Epoch 9] heading rmse[deg]: 103.977375\n",
      "2018-12-04 01:40:23.604066  [Epoch 9] class accuracy: 0.994105\n",
      "2018-12-04 01:40:23.605996  [Epoch 9] avg class acc: 0.993321\n",
      "2018-12-04 01:40:23.607667  [Epoch 9] indivisual [0] class recall: 0.993214\n",
      "2018-12-04 01:40:23.609334  [Epoch 9] indivisual [0] class precision: 0.993438\n",
      "2018-12-04 01:40:23.610885  [Epoch 9] indivisual [1] class recall: 0.995750\n",
      "2018-12-04 01:40:23.612535  [Epoch 9] indivisual [1] class precision: 0.996245\n",
      "2018-12-04 01:40:23.614463  [Epoch 9] indivisual [2] class recall: 0.990999\n",
      "2018-12-04 01:40:23.616565  [Epoch 9] indivisual [2] class precision: 0.988144\n",
      "2018-12-04 01:40:23.618341   Evaluation one (validation set) epoch  10 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:41:09.128539  [Epoch 9] Validation mean loss: 0.002120\n",
      "2018-12-04 01:41:09.130433  [Epoch 9] Validation heading rmse[deg]: 103.867211\n",
      "2018-12-04 01:41:09.132201  [Epoch 9] Validation class accuracy: 0.985167\n",
      "2018-12-04 01:41:09.134052  [Epoch 9] Validation avg class acc: 0.241780\n",
      "2018-12-04 01:41:09.135861  [Epoch 9] Validation indivisual [0] class recall: 0.246766\n",
      "2018-12-04 01:41:09.137535  [Epoch 9] Validation indivisual [0] class precision: 0.986315\n",
      "2018-12-04 01:41:09.139432  [Epoch 9] Validation indivisual [1] class recall: 0.249959\n",
      "2018-12-04 01:41:09.141291  [Epoch 9] Validation indivisual [1] class precision: 0.990248\n",
      "2018-12-04 01:41:09.142933  [Epoch 9] Validation indivisual [2] class recall: 0.228614\n",
      "2018-12-04 01:41:09.144649  [Epoch 9] Validation indivisual [2] class precision: 0.958741\n",
      "2018-12-04 01:41:10.054538  Model saved in file: /home/gozilla/kyungpyo/git/MasterThesisProject/Network/notebook/train_log_20181203/model_out5_1/model.ckpt\n",
      "2018-12-04 01:41:10.056218  \n",
      "\n",
      " Train one epoch  11 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:50:48.843257  [Epoch 10] mean loss: 0.000604\n",
      "2018-12-04 01:50:48.844990  [Epoch 10] heading rmse[deg]: 103.896439\n",
      "2018-12-04 01:50:48.847017  [Epoch 10] class accuracy: 0.995889\n",
      "2018-12-04 01:50:48.849248  [Epoch 10] avg class acc: 0.995245\n",
      "2018-12-04 01:50:48.850912  [Epoch 10] indivisual [0] class recall: 0.995439\n",
      "2018-12-04 01:50:48.852684  [Epoch 10] indivisual [0] class precision: 0.995243\n",
      "2018-12-04 01:50:48.854374  [Epoch 10] indivisual [1] class recall: 0.996964\n",
      "2018-12-04 01:50:48.856061  [Epoch 10] indivisual [1] class precision: 0.997350\n",
      "2018-12-04 01:50:48.857637  [Epoch 10] indivisual [2] class recall: 0.993333\n",
      "2018-12-04 01:50:48.859260  [Epoch 10] indivisual [2] class precision: 0.992561\n",
      "2018-12-04 01:50:48.860871   Evaluation one (validation set) epoch  11 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 01:51:34.370456  [Epoch 10] Validation mean loss: 0.002209\n",
      "2018-12-04 01:51:34.372215  [Epoch 10] Validation heading rmse[deg]: 103.817178\n",
      "2018-12-04 01:51:34.373923  [Epoch 10] Validation class accuracy: 0.987509\n",
      "2018-12-04 01:51:34.375668  [Epoch 10] Validation avg class acc: 0.243297\n",
      "2018-12-04 01:51:34.377247  [Epoch 10] Validation indivisual [0] class recall: 0.246883\n",
      "2018-12-04 01:51:34.379131  [Epoch 10] Validation indivisual [0] class precision: 0.991344\n",
      "2018-12-04 01:51:34.381043  [Epoch 10] Validation indivisual [1] class recall: 0.250078\n",
      "2018-12-04 01:51:34.382726  [Epoch 10] Validation indivisual [1] class precision: 0.988470\n",
      "2018-12-04 01:51:34.384405  [Epoch 10] Validation indivisual [2] class recall: 0.232930\n",
      "2018-12-04 01:51:34.386059  [Epoch 10] Validation indivisual [2] class precision: 0.967787\n",
      "2018-12-04 01:51:34.387784  \n",
      "\n",
      " Train one epoch  12 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:01:13.333821  [Epoch 11] mean loss: 0.000542\n",
      "2018-12-04 02:01:13.335631  [Epoch 11] heading rmse[deg]: 103.884182\n",
      "2018-12-04 02:01:13.337344  [Epoch 11] class accuracy: 0.996780\n",
      "2018-12-04 02:01:13.339063  [Epoch 11] avg class acc: 0.996193\n",
      "2018-12-04 02:01:13.340686  [Epoch 11] indivisual [0] class recall: 0.996565\n",
      "2018-12-04 02:01:13.342243  [Epoch 11] indivisual [0] class precision: 0.996116\n",
      "2018-12-04 02:01:13.343981  [Epoch 11] indivisual [1] class recall: 0.997572\n",
      "2018-12-04 02:01:13.345612  [Epoch 11] indivisual [1] class precision: 0.997930\n",
      "2018-12-04 02:01:13.347167  [Epoch 11] indivisual [2] class recall: 0.994443\n",
      "2018-12-04 02:01:13.349062  [Epoch 11] indivisual [2] class precision: 0.994775\n",
      "2018-12-04 02:01:13.351092   Evaluation one (validation set) epoch  12 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:01:58.850689  [Epoch 11] Validation mean loss: 0.002339\n",
      "2018-12-04 02:01:58.852696  [Epoch 11] Validation heading rmse[deg]: 103.830065\n",
      "2018-12-04 02:01:58.854479  [Epoch 11] Validation class accuracy: 0.986394\n",
      "2018-12-04 02:01:58.856351  [Epoch 11] Validation avg class acc: 0.242381\n",
      "2018-12-04 02:01:58.858044  [Epoch 11] Validation indivisual [0] class recall: 0.247099\n",
      "2018-12-04 02:01:58.859704  [Epoch 11] Validation indivisual [0] class precision: 0.988000\n",
      "2018-12-04 02:01:58.861602  [Epoch 11] Validation indivisual [1] class recall: 0.249948\n",
      "2018-12-04 02:01:58.863498  [Epoch 11] Validation indivisual [1] class precision: 0.989113\n",
      "2018-12-04 02:01:58.865246  [Epoch 11] Validation indivisual [2] class recall: 0.230096\n",
      "2018-12-04 02:01:58.866946  [Epoch 11] Validation indivisual [2] class precision: 0.968081\n",
      "2018-12-04 02:01:58.868601  \n",
      "\n",
      " Train one epoch  13 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:11:37.881317  [Epoch 12] mean loss: 0.000499\n",
      "2018-12-04 02:11:37.883070  [Epoch 12] heading rmse[deg]: 103.850279\n",
      "2018-12-04 02:11:37.884812  [Epoch 12] class accuracy: 0.997598\n",
      "2018-12-04 02:11:37.886509  [Epoch 12] avg class acc: 0.997140\n",
      "2018-12-04 02:11:37.888240  [Epoch 12] indivisual [0] class recall: 0.997382\n",
      "2018-12-04 02:11:37.889954  [Epoch 12] indivisual [0] class precision: 0.997157\n",
      "2018-12-04 02:11:37.891594  [Epoch 12] indivisual [1] class recall: 0.998261\n",
      "2018-12-04 02:11:37.893153  [Epoch 12] indivisual [1] class precision: 0.998564\n",
      "2018-12-04 02:11:37.895065  [Epoch 12] indivisual [2] class recall: 0.995776\n",
      "2018-12-04 02:11:37.897127  [Epoch 12] indivisual [2] class precision: 0.995444\n",
      "2018-12-04 02:11:37.898750   Evaluation one (validation set) epoch  13 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:12:23.458036  [Epoch 12] Validation mean loss: 0.002499\n",
      "2018-12-04 02:12:23.459930  [Epoch 12] Validation heading rmse[deg]: 103.797684\n",
      "2018-12-04 02:12:23.461644  [Epoch 12] Validation class accuracy: 0.985725\n",
      "2018-12-04 02:12:23.463465  [Epoch 12] Validation avg class acc: 0.242587\n",
      "2018-12-04 02:12:23.465101  [Epoch 12] Validation indivisual [0] class recall: 0.246482\n",
      "2018-12-04 02:12:23.466600  [Epoch 12] Validation indivisual [0] class precision: 0.988558\n",
      "2018-12-04 02:12:23.468231  [Epoch 12] Validation indivisual [1] class recall: 0.249850\n",
      "2018-12-04 02:12:23.469742  [Epoch 12] Validation indivisual [1] class precision: 0.989187\n",
      "2018-12-04 02:12:23.471264  [Epoch 12] Validation indivisual [2] class recall: 0.231430\n",
      "2018-12-04 02:12:23.472781  [Epoch 12] Validation indivisual [2] class precision: 0.959558\n",
      "2018-12-04 02:12:23.474225  \n",
      "\n",
      " Train one epoch  14 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:22:02.616363  [Epoch 13] mean loss: 0.000497\n",
      "2018-12-04 02:22:02.618335  [Epoch 13] heading rmse[deg]: 103.835983\n",
      "2018-12-04 02:22:02.621020  [Epoch 13] class accuracy: 0.997734\n",
      "2018-12-04 02:22:02.622865  [Epoch 13] avg class acc: 0.997326\n",
      "2018-12-04 02:22:02.624663  [Epoch 13] indivisual [0] class recall: 0.997550\n",
      "2018-12-04 02:22:02.626351  [Epoch 13] indivisual [0] class precision: 0.997298\n",
      "2018-12-04 02:22:02.628111  [Epoch 13] indivisual [1] class recall: 0.998317\n",
      "2018-12-04 02:22:02.629902  [Epoch 13] indivisual [1] class precision: 0.998647\n",
      "2018-12-04 02:22:02.631753  [Epoch 13] indivisual [2] class recall: 0.996111\n",
      "2018-12-04 02:22:02.633638  [Epoch 13] indivisual [2] class precision: 0.995779\n",
      "2018-12-04 02:22:02.635392   Evaluation one (validation set) epoch  14 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:22:48.033663  [Epoch 13] Validation mean loss: 0.002409\n",
      "2018-12-04 02:22:48.035455  [Epoch 13] Validation heading rmse[deg]: 104.061131\n",
      "2018-12-04 02:22:48.037134  [Epoch 13] Validation class accuracy: 0.986059\n",
      "2018-12-04 02:22:48.038801  [Epoch 13] Validation avg class acc: 0.242637\n",
      "2018-12-04 02:22:48.040754  [Epoch 13] Validation indivisual [0] class recall: 0.246429\n",
      "2018-12-04 02:22:48.042670  [Epoch 13] Validation indivisual [0] class precision: 0.990234\n",
      "2018-12-04 02:22:48.044447  [Epoch 13] Validation indivisual [1] class recall: 0.250098\n",
      "2018-12-04 02:22:48.046152  [Epoch 13] Validation indivisual [1] class precision: 0.988875\n",
      "2018-12-04 02:22:48.047848  [Epoch 13] Validation indivisual [2] class recall: 0.231385\n",
      "2018-12-04 02:22:48.049535  [Epoch 13] Validation indivisual [2] class precision: 0.957227\n",
      "2018-12-04 02:22:48.051181  \n",
      "\n",
      " Train one epoch  15 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:32:27.008785  [Epoch 14] mean loss: 0.000493\n",
      "2018-12-04 02:32:27.010698  [Epoch 14] heading rmse[deg]: 103.834745\n",
      "2018-12-04 02:32:27.012527  [Epoch 14] class accuracy: 0.997622\n",
      "2018-12-04 02:32:27.014398  [Epoch 14] avg class acc: 0.997408\n",
      "2018-12-04 02:32:27.016322  [Epoch 14] indivisual [0] class recall: 0.997240\n",
      "2018-12-04 02:32:27.018301  [Epoch 14] indivisual [0] class precision: 0.997381\n",
      "2018-12-04 02:32:27.020158  [Epoch 14] indivisual [1] class recall: 0.998206\n",
      "2018-12-04 02:32:27.021885  [Epoch 14] indivisual [1] class precision: 0.998261\n",
      "2018-12-04 02:32:27.023534  [Epoch 14] indivisual [2] class recall: 0.996778\n",
      "2018-12-04 02:32:27.025163  [Epoch 14] indivisual [2] class precision: 0.996003\n",
      "2018-12-04 02:32:27.026730   Evaluation one (validation set) epoch  15 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:33:12.520761  [Epoch 14] Validation mean loss: 0.002354\n",
      "2018-12-04 02:33:12.522545  [Epoch 14] Validation heading rmse[deg]: 103.821253\n",
      "2018-12-04 02:33:12.524254  [Epoch 14] Validation class accuracy: 0.986059\n",
      "2018-12-04 02:33:12.525933  [Epoch 14] Validation avg class acc: 0.242686\n",
      "2018-12-04 02:33:12.527545  [Epoch 14] Validation indivisual [0] class recall: 0.246444\n",
      "2018-12-04 02:33:12.529082  [Epoch 14] Validation indivisual [0] class precision: 0.989477\n",
      "2018-12-04 02:33:12.530619  [Epoch 14] Validation indivisual [1] class recall: 0.250036\n",
      "2018-12-04 02:33:12.532345  [Epoch 14] Validation indivisual [1] class precision: 0.989116\n",
      "2018-12-04 02:33:12.533893  [Epoch 14] Validation indivisual [2] class recall: 0.231577\n",
      "2018-12-04 02:33:12.535557  [Epoch 14] Validation indivisual [2] class precision: 0.959254\n",
      "2018-12-04 02:33:12.537182  \n",
      "\n",
      " Train one epoch  16 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:42:51.479473  [Epoch 15] mean loss: 0.000459\n",
      "2018-12-04 02:42:51.481618  [Epoch 15] heading rmse[deg]: 103.812878\n",
      "2018-12-04 02:42:51.483502  [Epoch 15] class accuracy: 0.998291\n",
      "2018-12-04 02:42:51.485359  [Epoch 15] avg class acc: 0.998079\n",
      "2018-12-04 02:42:51.486928  [Epoch 15] indivisual [0] class recall: 0.998311\n",
      "2018-12-04 02:42:51.488589  [Epoch 15] indivisual [0] class precision: 0.997833\n",
      "2018-12-04 02:42:51.490376  [Epoch 15] indivisual [1] class recall: 0.998482\n",
      "2018-12-04 02:42:51.492261  [Epoch 15] indivisual [1] class precision: 0.998951\n",
      "2018-12-04 02:42:51.494018  [Epoch 15] indivisual [2] class recall: 0.997444\n",
      "2018-12-04 02:42:51.495750  [Epoch 15] indivisual [2] class precision: 0.997444\n",
      "2018-12-04 02:42:51.497480   Evaluation one (validation set) epoch  16 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:43:36.944541  [Epoch 15] Validation mean loss: 0.002365\n",
      "2018-12-04 02:43:36.946367  [Epoch 15] Validation heading rmse[deg]: 103.786011\n",
      "2018-12-04 02:43:36.948139  [Epoch 15] Validation class accuracy: 0.986766\n",
      "2018-12-04 02:43:36.949975  [Epoch 15] Validation avg class acc: 0.242875\n",
      "2018-12-04 02:43:36.951688  [Epoch 15] Validation indivisual [0] class recall: 0.246814\n",
      "2018-12-04 02:43:36.953506  [Epoch 15] Validation indivisual [0] class precision: 0.989745\n",
      "2018-12-04 02:43:36.955371  [Epoch 15] Validation indivisual [1] class recall: 0.250010\n",
      "2018-12-04 02:43:36.957092  [Epoch 15] Validation indivisual [1] class precision: 0.989115\n",
      "2018-12-04 02:43:36.958684  [Epoch 15] Validation indivisual [2] class recall: 0.231802\n",
      "2018-12-04 02:43:36.960267  [Epoch 15] Validation indivisual [2] class precision: 0.964608\n",
      "2018-12-04 02:43:36.961907  \n",
      "\n",
      " Train one epoch  17 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:53:15.709577  [Epoch 16] mean loss: 0.000461\n",
      "2018-12-04 02:53:15.711433  [Epoch 16] heading rmse[deg]: 103.804839\n",
      "2018-12-04 02:53:15.713388  [Epoch 16] class accuracy: 0.998093\n",
      "2018-12-04 02:53:15.715582  [Epoch 16] avg class acc: 0.997736\n",
      "2018-12-04 02:53:15.717381  [Epoch 16] indivisual [0] class recall: 0.998114\n",
      "2018-12-04 02:53:15.719156  [Epoch 16] indivisual [0] class precision: 0.997580\n",
      "2018-12-04 02:53:15.720914  [Epoch 16] indivisual [1] class recall: 0.998427\n",
      "2018-12-04 02:53:15.722637  [Epoch 16] indivisual [1] class precision: 0.998813\n",
      "2018-12-04 02:53:15.724358  [Epoch 16] indivisual [2] class recall: 0.996666\n",
      "2018-12-04 02:53:15.726038  [Epoch 16] indivisual [2] class precision: 0.997220\n",
      "2018-12-04 02:53:15.727751   Evaluation one (validation set) epoch  17 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 02:54:01.283608  [Epoch 16] Validation mean loss: 0.002436\n",
      "2018-12-04 02:54:01.285503  [Epoch 16] Validation heading rmse[deg]: 103.769480\n",
      "2018-12-04 02:54:01.287201  [Epoch 16] Validation class accuracy: 0.986506\n",
      "2018-12-04 02:54:01.288897  [Epoch 16] Validation avg class acc: 0.242848\n",
      "2018-12-04 02:54:01.290485  [Epoch 16] Validation indivisual [0] class recall: 0.246534\n",
      "2018-12-04 02:54:01.292176  [Epoch 16] Validation indivisual [0] class precision: 0.990996\n",
      "2018-12-04 02:54:01.294051  [Epoch 16] Validation indivisual [1] class recall: 0.250119\n",
      "2018-12-04 02:54:01.295918  [Epoch 16] Validation indivisual [1] class precision: 0.987906\n",
      "2018-12-04 02:54:01.297648  [Epoch 16] Validation indivisual [2] class recall: 0.231891\n",
      "2018-12-04 02:54:01.299325  [Epoch 16] Validation indivisual [2] class precision: 0.962297\n",
      "2018-12-04 02:54:01.301092  \n",
      "\n",
      " Train one epoch  18 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:03:40.493819  [Epoch 17] mean loss: 0.000442\n",
      "2018-12-04 03:03:40.495927  [Epoch 17] heading rmse[deg]: 103.815674\n",
      "2018-12-04 03:03:40.498132  [Epoch 17] class accuracy: 0.998502\n",
      "2018-12-04 03:03:40.500106  [Epoch 17] avg class acc: 0.998486\n",
      "2018-12-04 03:03:40.501884  [Epoch 17] indivisual [0] class recall: 0.998367\n",
      "2018-12-04 03:03:40.503669  [Epoch 17] indivisual [0] class precision: 0.998254\n",
      "2018-12-04 03:03:40.505458  [Epoch 17] indivisual [1] class recall: 0.998648\n",
      "2018-12-04 03:03:40.507110  [Epoch 17] indivisual [1] class precision: 0.998868\n",
      "2018-12-04 03:03:40.508674  [Epoch 17] indivisual [2] class recall: 0.998444\n",
      "2018-12-04 03:03:40.510208  [Epoch 17] indivisual [2] class precision: 0.998001\n",
      "2018-12-04 03:03:40.511803   Evaluation one (validation set) epoch  18 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:04:25.950338  [Epoch 17] Validation mean loss: 0.002431\n",
      "2018-12-04 03:04:25.952334  [Epoch 17] Validation heading rmse[deg]: 103.837501\n",
      "2018-12-04 03:04:25.954149  [Epoch 17] Validation class accuracy: 0.986691\n",
      "2018-12-04 03:04:25.956222  [Epoch 17] Validation avg class acc: 0.243016\n",
      "2018-12-04 03:04:25.958177  [Epoch 17] Validation indivisual [0] class recall: 0.246418\n",
      "2018-12-04 03:04:25.960043  [Epoch 17] Validation indivisual [0] class precision: 0.991243\n",
      "2018-12-04 03:04:25.961816  [Epoch 17] Validation indivisual [1] class recall: 0.250202\n",
      "2018-12-04 03:04:25.963553  [Epoch 17] Validation indivisual [1] class precision: 0.987992\n",
      "2018-12-04 03:04:25.965303  [Epoch 17] Validation indivisual [2] class recall: 0.232430\n",
      "2018-12-04 03:04:25.966905  [Epoch 17] Validation indivisual [2] class precision: 0.962707\n",
      "2018-12-04 03:04:25.968565  \n",
      "\n",
      " Train one epoch  19 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:14:05.035491  [Epoch 18] mean loss: 0.000431\n",
      "2018-12-04 03:14:05.037883  [Epoch 18] heading rmse[deg]: 103.798997\n",
      "2018-12-04 03:14:05.039881  [Epoch 18] class accuracy: 0.998613\n",
      "2018-12-04 03:14:05.042101  [Epoch 18] avg class acc: 0.998431\n",
      "2018-12-04 03:14:05.043861  [Epoch 18] indivisual [0] class recall: 0.998480\n",
      "2018-12-04 03:14:05.045646  [Epoch 18] indivisual [0] class precision: 0.998367\n",
      "2018-12-04 03:14:05.047307  [Epoch 18] indivisual [1] class recall: 0.998924\n",
      "2018-12-04 03:14:05.048995  [Epoch 18] indivisual [1] class precision: 0.999062\n",
      "2018-12-04 03:14:05.050632  [Epoch 18] indivisual [2] class recall: 0.997889\n",
      "2018-12-04 03:14:05.052460  [Epoch 18] indivisual [2] class precision: 0.997778\n",
      "2018-12-04 03:14:05.054251   Evaluation one (validation set) epoch  19 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:14:50.527336  [Epoch 18] Validation mean loss: 0.002372\n",
      "2018-12-04 03:14:50.529160  [Epoch 18] Validation heading rmse[deg]: 103.786915\n",
      "2018-12-04 03:14:50.530930  [Epoch 18] Validation class accuracy: 0.986320\n",
      "2018-12-04 03:14:50.532845  [Epoch 18] Validation avg class acc: 0.243009\n",
      "2018-12-04 03:14:50.534612  [Epoch 18] Validation indivisual [0] class recall: 0.246244\n",
      "2018-12-04 03:14:50.536419  [Epoch 18] Validation indivisual [0] class precision: 0.991492\n",
      "2018-12-04 03:14:50.538048  [Epoch 18] Validation indivisual [1] class recall: 0.250103\n",
      "2018-12-04 03:14:50.539862  [Epoch 18] Validation indivisual [1] class precision: 0.988066\n",
      "2018-12-04 03:14:50.541737  [Epoch 18] Validation indivisual [2] class recall: 0.232680\n",
      "2018-12-04 03:14:50.543482  [Epoch 18] Validation indivisual [2] class precision: 0.958119\n",
      "2018-12-04 03:14:50.545266  \n",
      "\n",
      " Train one epoch  20 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:24:29.423224  [Epoch 19] mean loss: 0.000422\n",
      "2018-12-04 03:24:29.425177  [Epoch 19] heading rmse[deg]: 103.805623\n",
      "2018-12-04 03:24:29.427005  [Epoch 19] class accuracy: 0.998848\n",
      "2018-12-04 03:24:29.428843  [Epoch 19] avg class acc: 0.998802\n",
      "2018-12-04 03:24:29.430470  [Epoch 19] indivisual [0] class recall: 0.998789\n",
      "2018-12-04 03:24:29.432342  [Epoch 19] indivisual [0] class precision: 0.998593\n",
      "2018-12-04 03:24:29.434277  [Epoch 19] indivisual [1] class recall: 0.998951\n",
      "2018-12-04 03:24:29.436397  [Epoch 19] indivisual [1] class precision: 0.999255\n",
      "2018-12-04 03:24:29.438188  [Epoch 19] indivisual [2] class recall: 0.998667\n",
      "2018-12-04 03:24:29.439912  [Epoch 19] indivisual [2] class precision: 0.998223\n",
      "2018-12-04 03:24:29.441609   Evaluation one (validation set) epoch  20 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:25:14.949030  [Epoch 19] Validation mean loss: 0.002353\n",
      "2018-12-04 03:25:14.950886  [Epoch 19] Validation heading rmse[deg]: 103.814202\n",
      "2018-12-04 03:25:14.952472  [Epoch 19] Validation class accuracy: 0.986468\n",
      "2018-12-04 03:25:14.954179  [Epoch 19] Validation avg class acc: 0.242877\n",
      "2018-12-04 03:25:14.955964  [Epoch 19] Validation indivisual [0] class recall: 0.246587\n",
      "2018-12-04 03:25:14.957692  [Epoch 19] Validation indivisual [0] class precision: 0.990409\n",
      "2018-12-04 03:25:14.959393  [Epoch 19] Validation indivisual [1] class recall: 0.250010\n",
      "2018-12-04 03:25:14.960992  [Epoch 19] Validation indivisual [1] class precision: 0.988952\n",
      "2018-12-04 03:25:14.962803  [Epoch 19] Validation indivisual [2] class recall: 0.232033\n",
      "2018-12-04 03:25:14.964681  [Epoch 19] Validation indivisual [2] class precision: 0.959986\n",
      "2018-12-04 03:25:16.009190  Model saved in file: /home/gozilla/kyungpyo/git/MasterThesisProject/Network/notebook/train_log_20181203/model_out5_1/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "    log_path = os.path.abspath(os.path.join(model_save_path, 'train_log.txt') )\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(200000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 20\n",
    "\n",
    "    \"\"\" log file \"\"\"\n",
    "    LOG_FOUT = open(log_path, 'w')\n",
    "\n",
    "    def log_string(out_str):\n",
    "        out_str = str( datetime.datetime.fromtimestamp( time.time() ) ) + \"  \" + out_str\n",
    "        LOG_FOUT.write(out_str+'\\n')\n",
    "        LOG_FOUT.flush()\n",
    "        print(out_str)\n",
    "\n",
    "\n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" load traing model \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points, h_reg_weight=0.2)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            log_string ( \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "\n",
    "            log_string(  '[Epoch %d] mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            log_string ( \" Evaluation one (validation set) epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = False\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_true_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "                \n",
    "                \n",
    "                \n",
    "            log_string(  '[Epoch %d] Validation mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] Validation class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5_1',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_3:0\", shape=(), dtype=bool, device=/device:GPU:0)\n",
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n",
      "2018-12-04 03:25:25.101688  \n",
      "\n",
      " Train one epoch   1 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:39<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:35:06.308916  [Epoch 0] mean loss: 0.013274\n",
      "2018-12-04 03:35:06.310792  [Epoch 0] heading rmse[deg]: 142.914177\n",
      "2018-12-04 03:35:06.312612  [Epoch 0] class accuracy: 0.748830\n",
      "2018-12-04 03:35:06.314483  [Epoch 0] avg class acc: 0.780994\n",
      "2018-12-04 03:35:06.316344  [Epoch 0] indivisual [0] class recall: 0.681356\n",
      "2018-12-04 03:35:06.318611  [Epoch 0] indivisual [0] class precision: 0.734333\n",
      "2018-12-04 03:35:06.320603  [Epoch 0] indivisual [1] class recall: 0.782625\n",
      "2018-12-04 03:35:06.322349  [Epoch 0] indivisual [1] class precision: 0.740238\n",
      "2018-12-04 03:35:06.324285  [Epoch 0] indivisual [2] class recall: 0.879000\n",
      "2018-12-04 03:35:06.326085  [Epoch 0] indivisual [2] class precision: 0.833878\n",
      "2018-12-04 03:35:06.328034   Evaluation one (validation set) epoch   1 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:35:51.987930  [Epoch 0] Validation mean loss: 0.009940\n",
      "2018-12-04 03:35:51.989814  [Epoch 0] Validation heading rmse[deg]: 104.160129\n",
      "2018-12-04 03:35:51.991582  [Epoch 0] Validation class accuracy: 0.817138\n",
      "2018-12-04 03:35:51.993353  [Epoch 0] Validation avg class acc: 0.208969\n",
      "2018-12-04 03:35:51.994985  [Epoch 0] Validation indivisual [0] class recall: 0.216138\n",
      "2018-12-04 03:35:51.996593  [Epoch 0] Validation indivisual [0] class precision: 0.754204\n",
      "2018-12-04 03:35:51.998176  [Epoch 0] Validation indivisual [1] class recall: 0.187844\n",
      "2018-12-04 03:35:51.999801  [Epoch 0] Validation indivisual [1] class precision: 0.898654\n",
      "2018-12-04 03:35:52.001368  [Epoch 0] Validation indivisual [2] class recall: 0.222926\n",
      "2018-12-04 03:35:52.002897  [Epoch 0] Validation indivisual [2] class precision: 0.826329\n",
      "2018-12-04 03:35:52.004487  \n",
      "\n",
      " Train one epoch   2 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:45:30.938493  [Epoch 1] mean loss: 0.007450\n",
      "2018-12-04 03:45:30.940683  [Epoch 1] heading rmse[deg]: 105.346320\n",
      "2018-12-04 03:45:30.942806  [Epoch 1] class accuracy: 0.868508\n",
      "2018-12-04 03:45:30.944740  [Epoch 1] avg class acc: 0.884388\n",
      "2018-12-04 03:45:30.946564  [Epoch 1] indivisual [0] class recall: 0.843423\n",
      "2018-12-04 03:45:30.948385  [Epoch 1] indivisual [0] class precision: 0.856518\n",
      "2018-12-04 03:45:30.950127  [Epoch 1] indivisual [1] class recall: 0.877187\n",
      "2018-12-04 03:45:30.951855  [Epoch 1] indivisual [1] class precision: 0.879104\n",
      "2018-12-04 03:45:30.953541  [Epoch 1] indivisual [2] class recall: 0.932556\n",
      "2018-12-04 03:45:30.955289  [Epoch 1] indivisual [2] class precision: 0.872272\n",
      "2018-12-04 03:45:30.956907   Evaluation one (validation set) epoch   2 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:46:16.396105  [Epoch 1] Validation mean loss: 0.005382\n",
      "2018-12-04 03:46:16.398083  [Epoch 1] Validation heading rmse[deg]: 104.076190\n",
      "2018-12-04 03:46:16.399955  [Epoch 1] Validation class accuracy: 0.936654\n",
      "2018-12-04 03:46:16.401820  [Epoch 1] Validation avg class acc: 0.231166\n",
      "2018-12-04 03:46:16.403596  [Epoch 1] Validation indivisual [0] class recall: 0.242603\n",
      "2018-12-04 03:46:16.405223  [Epoch 1] Validation indivisual [0] class precision: 0.892474\n",
      "2018-12-04 03:46:16.407028  [Epoch 1] Validation indivisual [1] class recall: 0.228608\n",
      "2018-12-04 03:46:16.408942  [Epoch 1] Validation indivisual [1] class precision: 0.991921\n",
      "2018-12-04 03:46:16.410716  [Epoch 1] Validation indivisual [2] class recall: 0.222287\n",
      "2018-12-04 03:46:16.412366  [Epoch 1] Validation indivisual [2] class precision: 0.920290\n",
      "2018-12-04 03:46:16.414068  \n",
      "\n",
      " Train one epoch   3 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:55:55.341770  [Epoch 2] mean loss: 0.005466\n",
      "2018-12-04 03:55:55.343665  [Epoch 2] heading rmse[deg]: 104.590098\n",
      "2018-12-04 03:55:55.345173  [Epoch 2] class accuracy: 0.911913\n",
      "2018-12-04 03:55:55.346690  [Epoch 2] avg class acc: 0.919168\n",
      "2018-12-04 03:55:55.348201  [Epoch 2] indivisual [0] class recall: 0.894464\n",
      "2018-12-04 03:55:55.349654  [Epoch 2] indivisual [0] class precision: 0.904937\n",
      "2018-12-04 03:55:55.351027  [Epoch 2] indivisual [1] class recall: 0.921708\n",
      "2018-12-04 03:55:55.352433  [Epoch 2] indivisual [1] class precision: 0.926643\n",
      "2018-12-04 03:55:55.353856  [Epoch 2] indivisual [2] class recall: 0.941333\n",
      "2018-12-04 03:55:55.355310  [Epoch 2] indivisual [2] class precision: 0.882132\n",
      "2018-12-04 03:55:55.356745   Evaluation one (validation set) epoch   3 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 03:56:40.827911  [Epoch 2] Validation mean loss: 0.003940\n",
      "2018-12-04 03:56:40.829590  [Epoch 2] Validation heading rmse[deg]: 104.176466\n",
      "2018-12-04 03:56:40.830901  [Epoch 2] Validation class accuracy: 0.953160\n",
      "2018-12-04 03:56:40.832385  [Epoch 2] Validation avg class acc: 0.234319\n",
      "2018-12-04 03:56:40.833698  [Epoch 2] Validation indivisual [0] class recall: 0.231111\n",
      "2018-12-04 03:56:40.834903  [Epoch 2] Validation indivisual [0] class precision: 0.974252\n",
      "2018-12-04 03:56:40.836126  [Epoch 2] Validation indivisual [1] class recall: 0.248873\n",
      "2018-12-04 03:56:40.837338  [Epoch 2] Validation indivisual [1] class precision: 0.945978\n",
      "2018-12-04 03:56:40.838455  [Epoch 2] Validation indivisual [2] class recall: 0.222972\n",
      "2018-12-04 03:56:40.839752  [Epoch 2] Validation indivisual [2] class precision: 0.904022\n",
      "2018-12-04 03:56:40.841002  \n",
      "\n",
      " Train one epoch   4 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:06:19.493118  [Epoch 3] mean loss: 0.003821\n",
      "2018-12-04 04:06:19.494969  [Epoch 3] heading rmse[deg]: 104.401502\n",
      "2018-12-04 04:06:19.496721  [Epoch 3] class accuracy: 0.945238\n",
      "2018-12-04 04:06:19.498605  [Epoch 3] avg class acc: 0.947735\n",
      "2018-12-04 04:06:19.500444  [Epoch 3] indivisual [0] class recall: 0.932284\n",
      "2018-12-04 04:06:19.502441  [Epoch 3] indivisual [0] class precision: 0.942904\n",
      "2018-12-04 04:06:19.504317  [Epoch 3] indivisual [1] class recall: 0.955376\n",
      "2018-12-04 04:06:19.506102  [Epoch 3] indivisual [1] class precision: 0.961318\n",
      "2018-12-04 04:06:19.507965  [Epoch 3] indivisual [2] class recall: 0.955546\n",
      "2018-12-04 04:06:19.509763  [Epoch 3] indivisual [2] class precision: 0.893577\n",
      "2018-12-04 04:06:19.511467   Evaluation one (validation set) epoch   4 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:07:04.900067  [Epoch 3] Validation mean loss: 0.003715\n",
      "2018-12-04 04:07:04.901956  [Epoch 3] Validation heading rmse[deg]: 104.460234\n",
      "2018-12-04 04:07:04.903622  [Epoch 3] Validation class accuracy: 0.962230\n",
      "2018-12-04 04:07:04.905292  [Epoch 3] Validation avg class acc: 0.235755\n",
      "2018-12-04 04:07:04.906744  [Epoch 3] Validation indivisual [0] class recall: 0.241700\n",
      "2018-12-04 04:07:04.908262  [Epoch 3] Validation indivisual [0] class precision: 0.954037\n",
      "2018-12-04 04:07:04.910102  [Epoch 3] Validation indivisual [1] class recall: 0.243871\n",
      "2018-12-04 04:07:04.912168  [Epoch 3] Validation indivisual [1] class precision: 0.980617\n",
      "2018-12-04 04:07:04.913925  [Epoch 3] Validation indivisual [2] class recall: 0.221694\n",
      "2018-12-04 04:07:04.915655  [Epoch 3] Validation indivisual [2] class precision: 0.919751\n",
      "2018-12-04 04:07:04.917387  \n",
      "\n",
      " Train one epoch   5 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:16:43.394150  [Epoch 4] mean loss: 0.003438\n",
      "2018-12-04 04:16:43.396171  [Epoch 4] heading rmse[deg]: 104.336913\n",
      "2018-12-04 04:16:43.398036  [Epoch 4] class accuracy: 0.952656\n",
      "2018-12-04 04:16:43.400259  [Epoch 4] avg class acc: 0.954156\n",
      "2018-12-04 04:16:43.402275  [Epoch 4] indivisual [0] class recall: 0.939742\n",
      "2018-12-04 04:16:43.404120  [Epoch 4] indivisual [0] class precision: 0.952509\n",
      "2018-12-04 04:16:43.405926  [Epoch 4] indivisual [1] class recall: 0.963741\n",
      "2018-12-04 04:16:43.407683  [Epoch 4] indivisual [1] class precision: 0.967881\n",
      "2018-12-04 04:16:43.409415  [Epoch 4] indivisual [2] class recall: 0.958986\n",
      "2018-12-04 04:16:43.411062  [Epoch 4] indivisual [2] class precision: 0.896136\n",
      "2018-12-04 04:16:43.412689   Evaluation one (validation set) epoch   5 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:17:28.825875  [Epoch 4] Validation mean loss: 0.002805\n",
      "2018-12-04 04:17:28.827743  [Epoch 4] Validation heading rmse[deg]: 103.880178\n",
      "2018-12-04 04:17:28.829453  [Epoch 4] Validation class accuracy: 0.974684\n",
      "2018-12-04 04:17:28.831130  [Epoch 4] Validation avg class acc: 0.239707\n",
      "2018-12-04 04:17:28.832710  [Epoch 4] Validation indivisual [0] class recall: 0.243464\n",
      "2018-12-04 04:17:28.834258  [Epoch 4] Validation indivisual [0] class precision: 0.975856\n",
      "2018-12-04 04:17:28.836083  [Epoch 4] Validation indivisual [1] class recall: 0.247461\n",
      "2018-12-04 04:17:28.837691  [Epoch 4] Validation indivisual [1] class precision: 0.989823\n",
      "2018-12-04 04:17:28.839219  [Epoch 4] Validation indivisual [2] class recall: 0.228197\n",
      "2018-12-04 04:17:28.840774  [Epoch 4] Validation indivisual [2] class precision: 0.909302\n",
      "2018-12-04 04:17:28.842307  \n",
      "\n",
      " Train one epoch   6 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:27:07.676621  [Epoch 5] mean loss: 0.002610\n",
      "2018-12-04 04:27:07.678496  [Epoch 5] heading rmse[deg]: 104.077413\n",
      "2018-12-04 04:27:07.680341  [Epoch 5] class accuracy: 0.966316\n",
      "2018-12-04 04:27:07.682600  [Epoch 5] avg class acc: 0.967107\n",
      "2018-12-04 04:27:07.684865  [Epoch 5] indivisual [0] class recall: 0.955036\n",
      "2018-12-04 04:27:07.686656  [Epoch 5] indivisual [0] class precision: 0.968175\n",
      "2018-12-04 04:27:07.688418  [Epoch 5] indivisual [1] class recall: 0.976514\n",
      "2018-12-04 04:27:07.690139  [Epoch 5] indivisual [1] class precision: 0.979108\n",
      "2018-12-04 04:27:07.691898  [Epoch 5] indivisual [2] class recall: 0.969771\n",
      "2018-12-04 04:27:07.693545  [Epoch 5] indivisual [2] class precision: 0.911236\n",
      "2018-12-04 04:27:07.695244   Evaluation one (validation set) epoch   6 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:27:53.158414  [Epoch 5] Validation mean loss: 0.002950\n",
      "2018-12-04 04:27:53.160243  [Epoch 5] Validation heading rmse[deg]: 103.843341\n",
      "2018-12-04 04:27:53.161907  [Epoch 5] Validation class accuracy: 0.978773\n",
      "2018-12-04 04:27:53.163679  [Epoch 5] Validation avg class acc: 0.239192\n",
      "2018-12-04 04:27:53.165252  [Epoch 5] Validation indivisual [0] class recall: 0.245393\n",
      "2018-12-04 04:27:53.166750  [Epoch 5] Validation indivisual [0] class precision: 0.977935\n",
      "2018-12-04 04:27:53.168622  [Epoch 5] Validation indivisual [1] class recall: 0.249126\n",
      "2018-12-04 04:27:53.170292  [Epoch 5] Validation indivisual [1] class precision: 0.986401\n",
      "2018-12-04 04:27:53.171881  [Epoch 5] Validation indivisual [2] class recall: 0.223056\n",
      "2018-12-04 04:27:53.173735  [Epoch 5] Validation indivisual [2] class precision: 0.949273\n",
      "2018-12-04 04:27:53.175820  \n",
      "\n",
      " Train one epoch   7 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:37:32.227710  [Epoch 6] mean loss: 0.002344\n",
      "2018-12-04 04:37:32.229638  [Epoch 6] heading rmse[deg]: 104.158661\n",
      "2018-12-04 04:37:32.231431  [Epoch 6] class accuracy: 0.971071\n",
      "2018-12-04 04:37:32.233194  [Epoch 6] avg class acc: 0.971209\n",
      "2018-12-04 04:37:32.234795  [Epoch 6] indivisual [0] class recall: 0.961031\n",
      "2018-12-04 04:37:32.236371  [Epoch 6] indivisual [0] class precision: 0.973114\n",
      "2018-12-04 04:37:32.238025  [Epoch 6] indivisual [1] class recall: 0.980710\n",
      "2018-12-04 04:37:32.239657  [Epoch 6] indivisual [1] class precision: 0.983016\n",
      "2018-12-04 04:37:32.241219  [Epoch 6] indivisual [2] class recall: 0.971886\n",
      "2018-12-04 04:37:32.242674  [Epoch 6] indivisual [2] class precision: 0.918215\n",
      "2018-12-04 04:37:32.244325   Evaluation one (validation set) epoch   7 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:38:17.729265  [Epoch 6] Validation mean loss: 0.003035\n",
      "2018-12-04 04:38:17.731050  [Epoch 6] Validation heading rmse[deg]: 104.046673\n",
      "2018-12-04 04:38:17.732799  [Epoch 6] Validation class accuracy: 0.976580\n",
      "2018-12-04 04:38:17.734621  [Epoch 6] Validation avg class acc: 0.239089\n",
      "2018-12-04 04:38:17.736353  [Epoch 6] Validation indivisual [0] class recall: 0.244759\n",
      "2018-12-04 04:38:17.738248  [Epoch 6] Validation indivisual [0] class precision: 0.975737\n",
      "2018-12-04 04:38:17.740209  [Epoch 6] Validation indivisual [1] class recall: 0.248221\n",
      "2018-12-04 04:38:17.741972  [Epoch 6] Validation indivisual [1] class precision: 0.988139\n",
      "2018-12-04 04:38:17.743764  [Epoch 6] Validation indivisual [2] class recall: 0.224287\n",
      "2018-12-04 04:38:17.745478  [Epoch 6] Validation indivisual [2] class precision: 0.931464\n",
      "2018-12-04 04:38:17.747155  \n",
      "\n",
      " Train one epoch   8 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:47:56.687124  [Epoch 7] mean loss: 0.002054\n",
      "2018-12-04 04:47:56.688876  [Epoch 7] heading rmse[deg]: 103.988345\n",
      "2018-12-04 04:47:56.690421  [Epoch 7] class accuracy: 0.975715\n",
      "2018-12-04 04:47:56.692185  [Epoch 7] avg class acc: 0.975172\n",
      "2018-12-04 04:47:56.693944  [Epoch 7] indivisual [0] class recall: 0.967141\n",
      "2018-12-04 04:47:56.695513  [Epoch 7] indivisual [0] class precision: 0.977629\n",
      "2018-12-04 04:47:56.697038  [Epoch 7] indivisual [1] class recall: 0.984601\n",
      "2018-12-04 04:47:56.698560  [Epoch 7] indivisual [1] class precision: 0.986425\n",
      "2018-12-04 04:47:56.700348  [Epoch 7] indivisual [2] class recall: 0.973775\n",
      "2018-12-04 04:47:56.702375  [Epoch 7] indivisual [2] class precision: 0.927596\n",
      "2018-12-04 04:47:56.704168   Evaluation one (validation set) epoch   8 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:48:42.175428  [Epoch 7] Validation mean loss: 0.002986\n",
      "2018-12-04 04:48:42.177170  [Epoch 7] Validation heading rmse[deg]: 104.455771\n",
      "2018-12-04 04:48:42.178702  [Epoch 7] Validation class accuracy: 0.979145\n",
      "2018-12-04 04:48:42.180340  [Epoch 7] Validation avg class acc: 0.239281\n",
      "2018-12-04 04:48:42.181899  [Epoch 7] Validation indivisual [0] class recall: 0.246471\n",
      "2018-12-04 04:48:42.183537  [Epoch 7] Validation indivisual [0] class precision: 0.975981\n",
      "2018-12-04 04:48:42.185462  [Epoch 7] Validation indivisual [1] class recall: 0.248262\n",
      "2018-12-04 04:48:42.187262  [Epoch 7] Validation indivisual [1] class precision: 0.989121\n",
      "2018-12-04 04:48:42.189034  [Epoch 7] Validation indivisual [2] class recall: 0.223112\n",
      "2018-12-04 04:48:42.190697  [Epoch 7] Validation indivisual [2] class precision: 0.949610\n",
      "2018-12-04 04:48:42.192398  \n",
      "\n",
      " Train one epoch   9 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:58:20.657492  [Epoch 8] mean loss: 0.001806\n",
      "2018-12-04 04:58:20.659284  [Epoch 8] heading rmse[deg]: 103.964213\n",
      "2018-12-04 04:58:20.661324  [Epoch 8] class accuracy: 0.980285\n",
      "2018-12-04 04:58:20.663515  [Epoch 8] avg class acc: 0.978807\n",
      "2018-12-04 04:58:20.665493  [Epoch 8] indivisual [0] class recall: 0.973926\n",
      "2018-12-04 04:58:20.667320  [Epoch 8] indivisual [0] class precision: 0.981164\n",
      "2018-12-04 04:58:20.669048  [Epoch 8] indivisual [1] class recall: 0.987940\n",
      "2018-12-04 04:58:20.670712  [Epoch 8] indivisual [1] class precision: 0.989551\n",
      "2018-12-04 04:58:20.672402  [Epoch 8] indivisual [2] class recall: 0.974556\n",
      "2018-12-04 04:58:20.673968  [Epoch 8] indivisual [2] class precision: 0.940993\n",
      "2018-12-04 04:58:20.675611   Evaluation one (validation set) epoch   9 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 04:59:06.177760  [Epoch 8] Validation mean loss: 0.003120\n",
      "2018-12-04 04:59:06.179625  [Epoch 8] Validation heading rmse[deg]: 103.817252\n",
      "2018-12-04 04:59:06.181339  [Epoch 8] Validation class accuracy: 0.976134\n",
      "2018-12-04 04:59:06.183102  [Epoch 8] Validation avg class acc: 0.240189\n",
      "2018-12-04 04:59:06.184820  [Epoch 8] Validation indivisual [0] class recall: 0.245361\n",
      "2018-12-04 04:59:06.186521  [Epoch 8] Validation indivisual [0] class precision: 0.972359\n",
      "2018-12-04 04:59:06.188235  [Epoch 8] Validation indivisual [1] class recall: 0.246214\n",
      "2018-12-04 04:59:06.189994  [Epoch 8] Validation indivisual [1] class precision: 0.990842\n",
      "2018-12-04 04:59:06.191897  [Epoch 8] Validation indivisual [2] class recall: 0.228993\n",
      "2018-12-04 04:59:06.193856  [Epoch 8] Validation indivisual [2] class precision: 0.931502\n",
      "2018-12-04 04:59:06.195723  \n",
      "\n",
      " Train one epoch  10 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:08:44.599724  [Epoch 9] mean loss: 0.001668\n",
      "2018-12-04 05:08:44.601597  [Epoch 9] heading rmse[deg]: 103.954178\n",
      "2018-12-04 05:08:44.603234  [Epoch 9] class accuracy: 0.982489\n",
      "2018-12-04 05:08:44.605005  [Epoch 9] avg class acc: 0.980884\n",
      "2018-12-04 05:08:44.606699  [Epoch 9] indivisual [0] class recall: 0.977334\n",
      "2018-12-04 05:08:44.608791  [Epoch 9] indivisual [0] class precision: 0.982813\n",
      "2018-12-04 05:08:44.610804  [Epoch 9] indivisual [1] class recall: 0.989099\n",
      "2018-12-04 05:08:44.612526  [Epoch 9] indivisual [1] class precision: 0.990548\n",
      "2018-12-04 05:08:44.614242  [Epoch 9] indivisual [2] class recall: 0.976220\n",
      "2018-12-04 05:08:44.615944  [Epoch 9] indivisual [2] class precision: 0.949730\n",
      "2018-12-04 05:08:44.617655   Evaluation one (validation set) epoch  10 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:09:30.088625  [Epoch 9] Validation mean loss: 0.002701\n",
      "2018-12-04 05:09:30.090445  [Epoch 9] Validation heading rmse[deg]: 103.897215\n",
      "2018-12-04 05:09:30.092212  [Epoch 9] Validation class accuracy: 0.982379\n",
      "2018-12-04 05:09:30.093913  [Epoch 9] Validation avg class acc: 0.240693\n",
      "2018-12-04 05:09:30.095774  [Epoch 9] Validation indivisual [0] class recall: 0.245963\n",
      "2018-12-04 05:09:30.097668  [Epoch 9] Validation indivisual [0] class precision: 0.983936\n",
      "2018-12-04 05:09:30.099379  [Epoch 9] Validation indivisual [1] class recall: 0.249752\n",
      "2018-12-04 05:09:30.101039  [Epoch 9] Validation indivisual [1] class precision: 0.989915\n",
      "2018-12-04 05:09:30.102742  [Epoch 9] Validation indivisual [2] class recall: 0.226363\n",
      "2018-12-04 05:09:30.104393  [Epoch 9] Validation indivisual [2] class precision: 0.944019\n",
      "2018-12-04 05:09:31.029170  Model saved in file: /home/gozilla/kyungpyo/git/MasterThesisProject/Network/notebook/train_log_20181203/model_out5_2/model.ckpt\n",
      "2018-12-04 05:09:31.031281  \n",
      "\n",
      " Train one epoch  11 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:19:09.447008  [Epoch 10] mean loss: 0.001489\n",
      "2018-12-04 05:19:09.448839  [Epoch 10] heading rmse[deg]: 103.877445\n",
      "2018-12-04 05:19:09.450495  [Epoch 10] class accuracy: 0.986006\n",
      "2018-12-04 05:19:09.452376  [Epoch 10] avg class acc: 0.984083\n",
      "2018-12-04 05:19:09.454075  [Epoch 10] indivisual [0] class recall: 0.981642\n",
      "2018-12-04 05:19:09.456139  [Epoch 10] indivisual [0] class precision: 0.986503\n",
      "2018-12-04 05:19:09.458198  [Epoch 10] indivisual [1] class recall: 0.992162\n",
      "2018-12-04 05:19:09.459966  [Epoch 10] indivisual [1] class precision: 0.992902\n",
      "2018-12-04 05:19:09.461761  [Epoch 10] indivisual [2] class recall: 0.978444\n",
      "2018-12-04 05:19:09.463430  [Epoch 10] indivisual [2] class precision: 0.956966\n",
      "2018-12-04 05:19:09.465190   Evaluation one (validation set) epoch  11 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:19:54.893006  [Epoch 10] Validation mean loss: 0.002754\n",
      "2018-12-04 05:19:54.894828  [Epoch 10] Validation heading rmse[deg]: 103.804048\n",
      "2018-12-04 05:19:54.896664  [Epoch 10] Validation class accuracy: 0.982045\n",
      "2018-12-04 05:19:54.898668  [Epoch 10] Validation avg class acc: 0.240780\n",
      "2018-12-04 05:19:54.900636  [Epoch 10] Validation indivisual [0] class recall: 0.247052\n",
      "2018-12-04 05:19:54.902482  [Epoch 10] Validation indivisual [0] class precision: 0.980211\n",
      "2018-12-04 05:19:54.904248  [Epoch 10] Validation indivisual [1] class recall: 0.248360\n",
      "2018-12-04 05:19:54.905973  [Epoch 10] Validation indivisual [1] class precision: 0.989940\n",
      "2018-12-04 05:19:54.907687  [Epoch 10] Validation indivisual [2] class recall: 0.226928\n",
      "2018-12-04 05:19:54.909461  [Epoch 10] Validation indivisual [2] class precision: 0.956094\n",
      "2018-12-04 05:19:54.911113  \n",
      "\n",
      " Train one epoch  12 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:29:33.966921  [Epoch 11] mean loss: 0.001450\n",
      "2018-12-04 05:29:33.968842  [Epoch 11] heading rmse[deg]: 103.873706\n",
      "2018-12-04 05:29:33.970658  [Epoch 11] class accuracy: 0.986861\n",
      "2018-12-04 05:29:33.972633  [Epoch 11] avg class acc: 0.984699\n",
      "2018-12-04 05:29:33.974383  [Epoch 11] indivisual [0] class recall: 0.983331\n",
      "2018-12-04 05:29:33.976198  [Epoch 11] indivisual [0] class precision: 0.987083\n",
      "2018-12-04 05:29:33.977890  [Epoch 11] indivisual [1] class recall: 0.992439\n",
      "2018-12-04 05:29:33.979500  [Epoch 11] indivisual [1] class precision: 0.993316\n",
      "2018-12-04 05:29:33.981103  [Epoch 11] indivisual [2] class recall: 0.978329\n",
      "2018-12-04 05:29:33.982608  [Epoch 11] indivisual [2] class precision: 0.960502\n",
      "2018-12-04 05:29:33.984224   Evaluation one (validation set) epoch  12 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:30:19.414519  [Epoch 11] Validation mean loss: 0.002912\n",
      "2018-12-04 05:30:19.416516  [Epoch 11] Validation heading rmse[deg]: 103.826605\n",
      "2018-12-04 05:30:19.418311  [Epoch 11] Validation class accuracy: 0.983011\n",
      "2018-12-04 05:30:19.420196  [Epoch 11] Validation avg class acc: 0.241307\n",
      "2018-12-04 05:30:19.422026  [Epoch 11] Validation indivisual [0] class recall: 0.245958\n",
      "2018-12-04 05:30:19.423865  [Epoch 11] Validation indivisual [0] class precision: 0.985351\n",
      "2018-12-04 05:30:19.425500  [Epoch 11] Validation indivisual [1] class recall: 0.249617\n",
      "2018-12-04 05:30:19.427079  [Epoch 11] Validation indivisual [1] class precision: 0.989666\n",
      "2018-12-04 05:30:19.428797  [Epoch 11] Validation indivisual [2] class recall: 0.228345\n",
      "2018-12-04 05:30:19.430375  [Epoch 11] Validation indivisual [2] class precision: 0.945461\n",
      "2018-12-04 05:30:19.432068  \n",
      "\n",
      " Train one epoch  13 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:39:58.140898  [Epoch 12] mean loss: 0.001309\n",
      "2018-12-04 05:39:58.142836  [Epoch 12] heading rmse[deg]: 103.848136\n",
      "2018-12-04 05:39:58.144570  [Epoch 12] class accuracy: 0.988991\n",
      "2018-12-04 05:39:58.146415  [Epoch 12] avg class acc: 0.987107\n",
      "2018-12-04 05:39:58.148427  [Epoch 12] indivisual [0] class recall: 0.986148\n",
      "2018-12-04 05:39:58.150379  [Epoch 12] indivisual [0] class precision: 0.988960\n",
      "2018-12-04 05:39:58.152139  [Epoch 12] indivisual [1] class recall: 0.993625\n",
      "2018-12-04 05:39:58.153840  [Epoch 12] indivisual [1] class precision: 0.994091\n",
      "2018-12-04 05:39:58.155578  [Epoch 12] indivisual [2] class recall: 0.981549\n",
      "2018-12-04 05:39:58.157235  [Epoch 12] indivisual [2] class precision: 0.968843\n",
      "2018-12-04 05:39:58.158789   Evaluation one (validation set) epoch  13 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:40:43.664148  [Epoch 12] Validation mean loss: 0.002665\n",
      "2018-12-04 05:40:43.666048  [Epoch 12] Validation heading rmse[deg]: 103.794881\n",
      "2018-12-04 05:40:43.667833  [Epoch 12] Validation class accuracy: 0.984238\n",
      "2018-12-04 05:40:43.669615  [Epoch 12] Validation avg class acc: 0.241585\n",
      "2018-12-04 05:40:43.671287  [Epoch 12] Validation indivisual [0] class recall: 0.247158\n",
      "2018-12-04 05:40:43.672970  [Epoch 12] Validation indivisual [0] class precision: 0.984015\n",
      "2018-12-04 05:40:43.674557  [Epoch 12] Validation indivisual [1] class recall: 0.249084\n",
      "2018-12-04 05:40:43.676147  [Epoch 12] Validation indivisual [1] class precision: 0.989642\n",
      "2018-12-04 05:40:43.677680  [Epoch 12] Validation indivisual [2] class recall: 0.228512\n",
      "2018-12-04 05:40:43.679221  [Epoch 12] Validation indivisual [2] class precision: 0.962092\n",
      "2018-12-04 05:40:43.680770  \n",
      "\n",
      " Train one epoch  14 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:50:22.327448  [Epoch 13] mean loss: 0.001286\n",
      "2018-12-04 05:50:22.329252  [Epoch 13] heading rmse[deg]: 103.831275\n",
      "2018-12-04 05:50:22.330926  [Epoch 13] class accuracy: 0.989709\n",
      "2018-12-04 05:50:22.332719  [Epoch 13] avg class acc: 0.987872\n",
      "2018-12-04 05:50:22.334553  [Epoch 13] indivisual [0] class recall: 0.987132\n",
      "2018-12-04 05:50:22.336604  [Epoch 13] indivisual [0] class precision: 0.989528\n",
      "2018-12-04 05:50:22.338306  [Epoch 13] indivisual [1] class recall: 0.994039\n",
      "2018-12-04 05:50:22.339998  [Epoch 13] indivisual [1] class precision: 0.994698\n",
      "2018-12-04 05:50:22.341630  [Epoch 13] indivisual [2] class recall: 0.982444\n",
      "2018-12-04 05:50:22.343267  [Epoch 13] indivisual [2] class precision: 0.970582\n",
      "2018-12-04 05:50:22.344929   Evaluation one (validation set) epoch  14 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 05:51:07.854352  [Epoch 13] Validation mean loss: 0.002778\n",
      "2018-12-04 05:51:07.855960  [Epoch 13] Validation heading rmse[deg]: 103.988296\n",
      "2018-12-04 05:51:07.857446  [Epoch 13] Validation class accuracy: 0.985576\n",
      "2018-12-04 05:51:07.858971  [Epoch 13] Validation avg class acc: 0.241301\n",
      "2018-12-04 05:51:07.860415  [Epoch 13] Validation indivisual [0] class recall: 0.247422\n",
      "2018-12-04 05:51:07.861879  [Epoch 13] Validation indivisual [0] class precision: 0.985605\n",
      "2018-12-04 05:51:07.863332  [Epoch 13] Validation indivisual [1] class recall: 0.250098\n",
      "2018-12-04 05:51:07.864804  [Epoch 13] Validation indivisual [1] class precision: 0.989199\n",
      "2018-12-04 05:51:07.866104  [Epoch 13] Validation indivisual [2] class recall: 0.226382\n",
      "2018-12-04 05:51:07.867433  [Epoch 13] Validation indivisual [2] class precision: 0.969643\n",
      "2018-12-04 05:51:07.868767  \n",
      "\n",
      " Train one epoch  15 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:00:46.406486  [Epoch 14] mean loss: 0.001269\n",
      "2018-12-04 06:00:46.411329  [Epoch 14] heading rmse[deg]: 103.838148\n",
      "2018-12-04 06:00:46.413113  [Epoch 14] class accuracy: 0.989882\n",
      "2018-12-04 06:00:46.414868  [Epoch 14] avg class acc: 0.988198\n",
      "2018-12-04 06:00:46.416526  [Epoch 14] indivisual [0] class recall: 0.987497\n",
      "2018-12-04 06:00:46.418116  [Epoch 14] indivisual [0] class precision: 0.989531\n",
      "2018-12-04 06:00:46.419998  [Epoch 14] indivisual [1] class recall: 0.993874\n",
      "2018-12-04 06:00:46.421652  [Epoch 14] indivisual [1] class precision: 0.994505\n",
      "2018-12-04 06:00:46.423284  [Epoch 14] indivisual [2] class recall: 0.983222\n",
      "2018-12-04 06:00:46.424881  [Epoch 14] indivisual [2] class precision: 0.972845\n",
      "2018-12-04 06:00:46.426419   Evaluation one (validation set) epoch  15 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:01:31.968676  [Epoch 14] Validation mean loss: 0.002721\n",
      "2018-12-04 06:01:31.970619  [Epoch 14] Validation heading rmse[deg]: 103.817977\n",
      "2018-12-04 06:01:31.972430  [Epoch 14] Validation class accuracy: 0.984833\n",
      "2018-12-04 06:01:31.974305  [Epoch 14] Validation avg class acc: 0.241374\n",
      "2018-12-04 06:01:31.976212  [Epoch 14] Validation indivisual [0] class recall: 0.246761\n",
      "2018-12-04 06:01:31.978136  [Epoch 14] Validation indivisual [0] class precision: 0.986481\n",
      "2018-12-04 06:01:31.979894  [Epoch 14] Validation indivisual [1] class recall: 0.250119\n",
      "2018-12-04 06:01:31.981638  [Epoch 14] Validation indivisual [1] class precision: 0.989606\n",
      "2018-12-04 06:01:31.983312  [Epoch 14] Validation indivisual [2] class recall: 0.227242\n",
      "2018-12-04 06:01:31.984992  [Epoch 14] Validation indivisual [2] class precision: 0.957499\n",
      "2018-12-04 06:01:31.986608  \n",
      "\n",
      " Train one epoch  16 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:11:10.904691  [Epoch 15] mean loss: 0.001214\n",
      "2018-12-04 06:11:10.906617  [Epoch 15] heading rmse[deg]: 103.810659\n",
      "2018-12-04 06:11:10.908438  [Epoch 15] class accuracy: 0.991071\n",
      "2018-12-04 06:11:10.910364  [Epoch 15] avg class acc: 0.989423\n",
      "2018-12-04 06:11:10.912408  [Epoch 15] indivisual [0] class recall: 0.988849\n",
      "2018-12-04 06:11:10.914410  [Epoch 15] indivisual [0] class precision: 0.990942\n",
      "2018-12-04 06:11:10.916095  [Epoch 15] indivisual [1] class recall: 0.994867\n",
      "2018-12-04 06:11:10.917762  [Epoch 15] indivisual [1] class precision: 0.995169\n",
      "2018-12-04 06:11:10.919382  [Epoch 15] indivisual [2] class recall: 0.984554\n",
      "2018-12-04 06:11:10.921032  [Epoch 15] indivisual [2] class precision: 0.975234\n",
      "2018-12-04 06:11:10.922678   Evaluation one (validation set) epoch  16 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:11:56.416263  [Epoch 15] Validation mean loss: 0.002771\n",
      "2018-12-04 06:11:56.418188  [Epoch 15] Validation heading rmse[deg]: 103.782288\n",
      "2018-12-04 06:11:56.419947  [Epoch 15] Validation class accuracy: 0.984647\n",
      "2018-12-04 06:11:56.421685  [Epoch 15] Validation avg class acc: 0.241355\n",
      "2018-12-04 06:11:56.423374  [Epoch 15] Validation indivisual [0] class recall: 0.247046\n",
      "2018-12-04 06:11:56.425057  [Epoch 15] Validation indivisual [0] class precision: 0.984917\n",
      "2018-12-04 06:11:56.426936  [Epoch 15] Validation indivisual [1] class recall: 0.249721\n",
      "2018-12-04 06:11:56.428848  [Epoch 15] Validation indivisual [1] class precision: 0.989670\n",
      "2018-12-04 06:11:56.430600  [Epoch 15] Validation indivisual [2] class recall: 0.227299\n",
      "2018-12-04 06:11:56.432363  [Epoch 15] Validation indivisual [2] class precision: 0.961891\n",
      "2018-12-04 06:11:56.434093  \n",
      "\n",
      " Train one epoch  17 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:21:34.725103  [Epoch 16] mean loss: 0.001188\n",
      "2018-12-04 06:21:34.727039  [Epoch 16] heading rmse[deg]: 103.814678\n",
      "2018-12-04 06:21:34.728776  [Epoch 16] class accuracy: 0.991307\n",
      "2018-12-04 06:21:34.730777  [Epoch 16] avg class acc: 0.989850\n",
      "2018-12-04 06:21:34.732795  [Epoch 16] indivisual [0] class recall: 0.988963\n",
      "2018-12-04 06:21:34.734697  [Epoch 16] indivisual [0] class precision: 0.991307\n",
      "2018-12-04 06:21:34.736485  [Epoch 16] indivisual [1] class recall: 0.995032\n",
      "2018-12-04 06:21:34.738151  [Epoch 16] indivisual [1] class precision: 0.995527\n",
      "2018-12-04 06:21:34.739788  [Epoch 16] indivisual [2] class recall: 0.985554\n",
      "2018-12-04 06:21:34.741445  [Epoch 16] indivisual [2] class precision: 0.974508\n",
      "2018-12-04 06:21:34.743059   Evaluation one (validation set) epoch  17 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:22:20.191478  [Epoch 16] Validation mean loss: 0.002753\n",
      "2018-12-04 06:22:20.193412  [Epoch 16] Validation heading rmse[deg]: 103.779570\n",
      "2018-12-04 06:22:20.195137  [Epoch 16] Validation class accuracy: 0.985242\n",
      "2018-12-04 06:22:20.197041  [Epoch 16] Validation avg class acc: 0.241656\n",
      "2018-12-04 06:22:20.198995  [Epoch 16] Validation indivisual [0] class recall: 0.246999\n",
      "2018-12-04 06:22:20.200940  [Epoch 16] Validation indivisual [0] class precision: 0.986745\n",
      "2018-12-04 06:22:20.202749  [Epoch 16] Validation indivisual [1] class recall: 0.249912\n",
      "2018-12-04 06:22:20.204545  [Epoch 16] Validation indivisual [1] class precision: 0.989271\n",
      "2018-12-04 06:22:20.206372  [Epoch 16] Validation indivisual [2] class recall: 0.228057\n",
      "2018-12-04 06:22:20.208189  [Epoch 16] Validation indivisual [2] class precision: 0.961687\n",
      "2018-12-04 06:22:20.209920  \n",
      "\n",
      " Train one epoch  18 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:31:58.341477  [Epoch 17] mean loss: 0.001180\n",
      "2018-12-04 06:31:58.343415  [Epoch 17] heading rmse[deg]: 103.821142\n",
      "2018-12-04 06:31:58.345223  [Epoch 17] class accuracy: 0.991356\n",
      "2018-12-04 06:31:58.347022  [Epoch 17] avg class acc: 0.990002\n",
      "2018-12-04 06:31:58.348657  [Epoch 17] indivisual [0] class recall: 0.989610\n",
      "2018-12-04 06:31:58.350580  [Epoch 17] indivisual [0] class precision: 0.990782\n",
      "2018-12-04 06:31:58.352645  [Epoch 17] indivisual [1] class recall: 0.994398\n",
      "2018-12-04 06:31:58.354351  [Epoch 17] indivisual [1] class precision: 0.995414\n",
      "2018-12-04 06:31:58.356138  [Epoch 17] indivisual [2] class recall: 0.985998\n",
      "2018-12-04 06:31:58.357917  [Epoch 17] indivisual [2] class precision: 0.977418\n",
      "2018-12-04 06:31:58.359614   Evaluation one (validation set) epoch  18 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:32:43.777788  [Epoch 17] Validation mean loss: 0.002677\n",
      "2018-12-04 06:32:43.779778  [Epoch 17] Validation heading rmse[deg]: 103.850305\n",
      "2018-12-04 06:32:43.781682  [Epoch 17] Validation class accuracy: 0.986989\n",
      "2018-12-04 06:32:43.783785  [Epoch 17] Validation avg class acc: 0.242036\n",
      "2018-12-04 06:32:43.785559  [Epoch 17] Validation indivisual [0] class recall: 0.247686\n",
      "2018-12-04 06:32:43.787362  [Epoch 17] Validation indivisual [0] class precision: 0.987695\n",
      "2018-12-04 06:32:43.789122  [Epoch 17] Validation indivisual [1] class recall: 0.250160\n",
      "2018-12-04 06:32:43.790827  [Epoch 17] Validation indivisual [1] class precision: 0.989364\n",
      "2018-12-04 06:32:43.792496  [Epoch 17] Validation indivisual [2] class recall: 0.228262\n",
      "2018-12-04 06:32:43.794121  [Epoch 17] Validation indivisual [2] class precision: 0.973684\n",
      "2018-12-04 06:32:43.795795  \n",
      "\n",
      " Train one epoch  19 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:42:22.180151  [Epoch 18] mean loss: 0.001137\n",
      "2018-12-04 06:42:22.182177  [Epoch 18] heading rmse[deg]: 103.808694\n",
      "2018-12-04 06:42:22.184124  [Epoch 18] class accuracy: 0.992173\n",
      "2018-12-04 06:42:22.186070  [Epoch 18] avg class acc: 0.990640\n",
      "2018-12-04 06:42:22.187882  [Epoch 18] indivisual [0] class recall: 0.990004\n",
      "2018-12-04 06:42:22.189614  [Epoch 18] indivisual [0] class precision: 0.992239\n",
      "2018-12-04 06:42:22.191260  [Epoch 18] indivisual [1] class recall: 0.995805\n",
      "2018-12-04 06:42:22.192870  [Epoch 18] indivisual [1] class precision: 0.995778\n",
      "2018-12-04 06:42:22.194737  [Epoch 18] indivisual [2] class recall: 0.986111\n",
      "2018-12-04 06:42:22.196742  [Epoch 18] indivisual [2] class precision: 0.977531\n",
      "2018-12-04 06:42:22.198399   Evaluation one (validation set) epoch  19 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:43:07.745566  [Epoch 18] Validation mean loss: 0.002735\n",
      "2018-12-04 06:43:07.747532  [Epoch 18] Validation heading rmse[deg]: 103.802677\n",
      "2018-12-04 06:43:07.749333  [Epoch 18] Validation class accuracy: 0.986617\n",
      "2018-12-04 06:43:07.751147  [Epoch 18] Validation avg class acc: 0.242011\n",
      "2018-12-04 06:43:07.752899  [Epoch 18] Validation indivisual [0] class recall: 0.247956\n",
      "2018-12-04 06:43:07.754684  [Epoch 18] Validation indivisual [0] class precision: 0.986133\n",
      "2018-12-04 06:43:07.756563  [Epoch 18] Validation indivisual [1] class recall: 0.249648\n",
      "2018-12-04 06:43:07.758504  [Epoch 18] Validation indivisual [1] class precision: 0.989503\n",
      "2018-12-04 06:43:07.760341  [Epoch 18] Validation indivisual [2] class recall: 0.228429\n",
      "2018-12-04 06:43:07.762021  [Epoch 18] Validation indivisual [2] class precision: 0.976131\n",
      "2018-12-04 06:43:07.763686  \n",
      "\n",
      " Train one epoch  20 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:52:46.040525  [Epoch 19] mean loss: 0.001132\n",
      "2018-12-04 06:52:46.042443  [Epoch 19] heading rmse[deg]: 103.809524\n",
      "2018-12-04 06:52:46.044216  [Epoch 19] class accuracy: 0.992582\n",
      "2018-12-04 06:52:46.046067  [Epoch 19] avg class acc: 0.991476\n",
      "2018-12-04 06:52:46.047683  [Epoch 19] indivisual [0] class recall: 0.990512\n",
      "2018-12-04 06:52:46.049334  [Epoch 19] indivisual [0] class precision: 0.992608\n",
      "2018-12-04 06:52:46.050905  [Epoch 19] indivisual [1] class recall: 0.995695\n",
      "2018-12-04 06:52:46.052437  [Epoch 19] indivisual [1] class precision: 0.996134\n",
      "2018-12-04 06:52:46.053940  [Epoch 19] indivisual [2] class recall: 0.988222\n",
      "2018-12-04 06:52:46.055440  [Epoch 19] indivisual [2] class precision: 0.978330\n",
      "2018-12-04 06:52:46.057055   Evaluation one (validation set) epoch  20 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:44<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 06:53:31.436071  [Epoch 19] Validation mean loss: 0.002703\n",
      "2018-12-04 06:53:31.438098  [Epoch 19] Validation heading rmse[deg]: 103.821200\n",
      "2018-12-04 06:53:31.440073  [Epoch 19] Validation class accuracy: 0.986952\n",
      "2018-12-04 06:53:31.442049  [Epoch 19] Validation avg class acc: 0.241973\n",
      "2018-12-04 06:53:31.443813  [Epoch 19] Validation indivisual [0] class recall: 0.247919\n",
      "2018-12-04 06:53:31.445534  [Epoch 19] Validation indivisual [0] class precision: 0.987127\n",
      "2018-12-04 06:53:31.447087  [Epoch 19] Validation indivisual [1] class recall: 0.249969\n",
      "2018-12-04 06:53:31.448638  [Epoch 19] Validation indivisual [1] class precision: 0.989679\n",
      "2018-12-04 06:53:31.450138  [Epoch 19] Validation indivisual [2] class recall: 0.228031\n",
      "2018-12-04 06:53:31.451682  [Epoch 19] Validation indivisual [2] class precision: 0.974350\n",
      "2018-12-04 06:53:32.520802  Model saved in file: /home/gozilla/kyungpyo/git/MasterThesisProject/Network/notebook/train_log_20181203/model_out5_2/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "    log_path = os.path.abspath(os.path.join(model_save_path, 'train_log.txt') )\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(200000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 20\n",
    "\n",
    "    \"\"\" log file \"\"\"\n",
    "    LOG_FOUT = open(log_path, 'w')\n",
    "\n",
    "    def log_string(out_str):\n",
    "        out_str = str( datetime.datetime.fromtimestamp( time.time() ) ) + \"  \" + out_str\n",
    "        LOG_FOUT.write(out_str+'\\n')\n",
    "        LOG_FOUT.flush()\n",
    "        print(out_str)\n",
    "\n",
    "\n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" load traing model \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points, h_reg_weight=0.4)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            log_string ( \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "\n",
    "            log_string(  '[Epoch %d] mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            log_string ( \" Evaluation one (validation set) epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = False\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_true_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "                \n",
    "                \n",
    "                \n",
    "            log_string(  '[Epoch %d] Validation mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] Validation class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5_2',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_3:0\", shape=(), dtype=bool, device=/device:GPU:0)\n",
      "INFO:tensorflow:Summary name classify loss is illegal; using classify_loss instead.\n",
      "INFO:tensorflow:Summary name mat loss is illegal; using mat_loss instead.\n",
      "2018-12-04 06:53:41.668940  \n",
      "\n",
      " Train one epoch   1 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:41<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:03:24.244352  [Epoch 0] mean loss: 0.015055\n",
      "2018-12-04 07:03:24.246387  [Epoch 0] heading rmse[deg]: 152.989468\n",
      "2018-12-04 07:03:24.248322  [Epoch 0] class accuracy: 0.753127\n",
      "2018-12-04 07:03:24.250278  [Epoch 0] avg class acc: 0.778715\n",
      "2018-12-04 07:03:24.252153  [Epoch 0] indivisual [0] class recall: 0.687467\n",
      "2018-12-04 07:03:24.253978  [Epoch 0] indivisual [0] class precision: 0.740941\n",
      "2018-12-04 07:03:24.255707  [Epoch 0] indivisual [1] class recall: 0.791677\n",
      "2018-12-04 07:03:24.257389  [Epoch 0] indivisual [1] class precision: 0.744659\n",
      "2018-12-04 07:03:24.259058  [Epoch 0] indivisual [2] class recall: 0.857000\n",
      "2018-12-04 07:03:24.261043  [Epoch 0] indivisual [2] class precision: 0.831590\n",
      "2018-12-04 07:03:24.263181   Evaluation one (validation set) epoch   1 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:46<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:04:10.790909  [Epoch 0] Validation mean loss: 0.011185\n",
      "2018-12-04 07:04:10.792856  [Epoch 0] Validation heading rmse[deg]: 106.024443\n",
      "2018-12-04 07:04:10.794681  [Epoch 0] Validation class accuracy: 0.840112\n",
      "2018-12-04 07:04:10.796857  [Epoch 0] Validation avg class acc: 0.214528\n",
      "2018-12-04 07:04:10.798533  [Epoch 0] Validation indivisual [0] class recall: 0.198597\n",
      "2018-12-04 07:04:10.800274  [Epoch 0] Validation indivisual [0] class precision: 0.839618\n",
      "2018-12-04 07:04:10.801936  [Epoch 0] Validation indivisual [1] class recall: 0.216393\n",
      "2018-12-04 07:04:10.803619  [Epoch 0] Validation indivisual [1] class precision: 0.842665\n",
      "2018-12-04 07:04:10.805226  [Epoch 0] Validation indivisual [2] class recall: 0.228595\n",
      "2018-12-04 07:04:10.806832  [Epoch 0] Validation indivisual [2] class precision: 0.832170\n",
      "2018-12-04 07:04:10.808415  \n",
      "\n",
      " Train one epoch   2 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:13:50.691747  [Epoch 1] mean loss: 0.007609\n",
      "2018-12-04 07:13:50.693690  [Epoch 1] heading rmse[deg]: 105.850996\n",
      "2018-12-04 07:13:50.695412  [Epoch 1] class accuracy: 0.878811\n",
      "2018-12-04 07:13:50.697409  [Epoch 1] avg class acc: 0.890118\n",
      "2018-12-04 07:13:50.699070  [Epoch 1] indivisual [0] class recall: 0.851278\n",
      "2018-12-04 07:13:50.700608  [Epoch 1] indivisual [0] class precision: 0.870920\n",
      "2018-12-04 07:13:50.702391  [Epoch 1] indivisual [1] class recall: 0.894409\n",
      "2018-12-04 07:13:50.704100  [Epoch 1] indivisual [1] class precision: 0.888109\n",
      "2018-12-04 07:13:50.705884  [Epoch 1] indivisual [2] class recall: 0.924667\n",
      "2018-12-04 07:13:50.707431  [Epoch 1] indivisual [2] class precision: 0.871961\n",
      "2018-12-04 07:13:50.709057   Evaluation one (validation set) epoch   2 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:14:36.939481  [Epoch 1] Validation mean loss: 0.005731\n",
      "2018-12-04 07:14:36.941691  [Epoch 1] Validation heading rmse[deg]: 104.153157\n",
      "2018-12-04 07:14:36.943480  [Epoch 1] Validation class accuracy: 0.924833\n",
      "2018-12-04 07:14:36.945468  [Epoch 1] Validation avg class acc: 0.227516\n",
      "2018-12-04 07:14:36.947196  [Epoch 1] Validation indivisual [0] class recall: 0.219503\n",
      "2018-12-04 07:14:36.949060  [Epoch 1] Validation indivisual [0] class precision: 0.953456\n",
      "2018-12-04 07:14:36.950948  [Epoch 1] Validation indivisual [1] class recall: 0.245924\n",
      "2018-12-04 07:14:36.952616  [Epoch 1] Validation indivisual [1] class precision: 0.901486\n",
      "2018-12-04 07:14:36.954155  [Epoch 1] Validation indivisual [2] class recall: 0.217120\n",
      "2018-12-04 07:14:36.955942  [Epoch 1] Validation indivisual [2] class precision: 0.923431\n",
      "2018-12-04 07:14:36.957725  \n",
      "\n",
      " Train one epoch   3 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:24:16.773474  [Epoch 2] mean loss: 0.005227\n",
      "2018-12-04 07:24:16.775954  [Epoch 2] heading rmse[deg]: 104.697977\n",
      "2018-12-04 07:24:16.778303  [Epoch 2] class accuracy: 0.927542\n",
      "2018-12-04 07:24:16.780778  [Epoch 2] avg class acc: 0.931490\n",
      "2018-12-04 07:24:16.782899  [Epoch 2] indivisual [0] class recall: 0.905305\n",
      "2018-12-04 07:24:16.785051  [Epoch 2] indivisual [0] class precision: 0.928495\n",
      "2018-12-04 07:24:16.787266  [Epoch 2] indivisual [1] class recall: 0.945275\n",
      "2018-12-04 07:24:16.789115  [Epoch 2] indivisual [1] class precision: 0.937642\n",
      "2018-12-04 07:24:16.791054  [Epoch 2] indivisual [2] class recall: 0.943889\n",
      "2018-12-04 07:24:16.792971  [Epoch 2] indivisual [2] class precision: 0.885634\n",
      "2018-12-04 07:24:16.795000   Evaluation one (validation set) epoch   3 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:25:02.842991  [Epoch 2] Validation mean loss: 0.006595\n",
      "2018-12-04 07:25:02.844896  [Epoch 2] Validation heading rmse[deg]: 104.167053\n",
      "2018-12-04 07:25:02.846609  [Epoch 2] Validation class accuracy: 0.902416\n",
      "2018-12-04 07:25:02.848390  [Epoch 2] Validation avg class acc: 0.225331\n",
      "2018-12-04 07:25:02.850077  [Epoch 2] Validation indivisual [0] class recall: 0.205495\n",
      "2018-12-04 07:25:02.851926  [Epoch 2] Validation indivisual [0] class precision: 0.954827\n",
      "2018-12-04 07:25:02.853847  [Epoch 2] Validation indivisual [1] class recall: 0.245107\n",
      "2018-12-04 07:25:02.855650  [Epoch 2] Validation indivisual [1] class precision: 0.867604\n",
      "2018-12-04 07:25:02.857360  [Epoch 2] Validation indivisual [2] class recall: 0.225390\n",
      "2018-12-04 07:25:02.858958  [Epoch 2] Validation indivisual [2] class precision: 0.883371\n",
      "2018-12-04 07:25:02.860850  \n",
      "\n",
      " Train one epoch   4 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:34:42.562409  [Epoch 3] mean loss: 0.004002\n",
      "2018-12-04 07:34:42.564254  [Epoch 3] heading rmse[deg]: 104.595677\n",
      "2018-12-04 07:34:42.565844  [Epoch 3] class accuracy: 0.953598\n",
      "2018-12-04 07:34:42.567438  [Epoch 3] avg class acc: 0.953829\n",
      "2018-12-04 07:34:42.568979  [Epoch 3] indivisual [0] class recall: 0.939999\n",
      "2018-12-04 07:34:42.570553  [Epoch 3] indivisual [0] class precision: 0.954048\n",
      "2018-12-04 07:34:42.572273  [Epoch 3] indivisual [1] class recall: 0.966608\n",
      "2018-12-04 07:34:42.573783  [Epoch 3] indivisual [1] class precision: 0.965968\n",
      "2018-12-04 07:34:42.575388  [Epoch 3] indivisual [2] class recall: 0.954879\n",
      "2018-12-04 07:34:42.577211  [Epoch 3] indivisual [2] class precision: 0.904707\n",
      "2018-12-04 07:34:42.578848   Evaluation one (validation set) epoch   4 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:35:28.221441  [Epoch 3] Validation mean loss: 0.003277\n",
      "2018-12-04 07:35:28.223263  [Epoch 3] Validation heading rmse[deg]: 104.887514\n",
      "2018-12-04 07:35:28.224985  [Epoch 3] Validation class accuracy: 0.976952\n",
      "2018-12-04 07:35:28.226685  [Epoch 3] Validation avg class acc: 0.238700\n",
      "2018-12-04 07:35:28.228327  [Epoch 3] Validation indivisual [0] class recall: 0.245377\n",
      "2018-12-04 07:35:28.230149  [Epoch 3] Validation indivisual [0] class precision: 0.970738\n",
      "2018-12-04 07:35:28.232061  [Epoch 3] Validation indivisual [1] class recall: 0.248278\n",
      "2018-12-04 07:35:28.234090  [Epoch 3] Validation indivisual [1] class precision: 0.990917\n",
      "2018-12-04 07:35:28.235854  [Epoch 3] Validation indivisual [2] class recall: 0.222445\n",
      "2018-12-04 07:35:28.237605  [Epoch 3] Validation indivisual [2] class precision: 0.943423\n",
      "2018-12-04 07:35:28.239279  \n",
      "\n",
      " Train one epoch   5 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:45:08.002583  [Epoch 4] mean loss: 0.003298\n",
      "2018-12-04 07:45:08.004543  [Epoch 4] heading rmse[deg]: 104.527389\n",
      "2018-12-04 07:45:08.006394  [Epoch 4] class accuracy: 0.965796\n",
      "2018-12-04 07:45:08.008326  [Epoch 4] avg class acc: 0.964830\n",
      "2018-12-04 07:45:08.010058  [Epoch 4] indivisual [0] class recall: 0.955708\n",
      "2018-12-04 07:45:08.012094  [Epoch 4] indivisual [0] class precision: 0.966264\n",
      "2018-12-04 07:45:08.014131  [Epoch 4] indivisual [1] class recall: 0.976572\n",
      "2018-12-04 07:45:08.015856  [Epoch 4] indivisual [1] class precision: 0.977327\n",
      "2018-12-04 07:45:08.017509  [Epoch 4] indivisual [2] class recall: 0.962210\n",
      "2018-12-04 07:45:08.019130  [Epoch 4] indivisual [2] class precision: 0.919686\n",
      "2018-12-04 07:45:08.020825   Evaluation one (validation set) epoch   5 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:45:53.534547  [Epoch 4] Validation mean loss: 0.003050\n",
      "2018-12-04 07:45:53.536426  [Epoch 4] Validation heading rmse[deg]: 104.067663\n",
      "2018-12-04 07:45:53.538155  [Epoch 4] Validation class accuracy: 0.976468\n",
      "2018-12-04 07:45:53.539908  [Epoch 4] Validation avg class acc: 0.240123\n",
      "2018-12-04 07:45:53.541460  [Epoch 4] Validation indivisual [0] class recall: 0.243612\n",
      "2018-12-04 07:45:53.542925  [Epoch 4] Validation indivisual [0] class precision: 0.975209\n",
      "2018-12-04 07:45:53.544466  [Epoch 4] Validation indivisual [1] class recall: 0.248226\n",
      "2018-12-04 07:45:53.545965  [Epoch 4] Validation indivisual [1] class precision: 0.992474\n",
      "2018-12-04 07:45:53.547528  [Epoch 4] Validation indivisual [2] class recall: 0.228531\n",
      "2018-12-04 07:45:53.549037  [Epoch 4] Validation indivisual [2] class precision: 0.916722\n",
      "2018-12-04 07:45:53.550466  \n",
      "\n",
      " Train one epoch   6 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:55:33.350596  [Epoch 5] mean loss: 0.002486\n",
      "2018-12-04 07:55:33.352564  [Epoch 5] heading rmse[deg]: 104.128122\n",
      "2018-12-04 07:55:33.354389  [Epoch 5] class accuracy: 0.979765\n",
      "2018-12-04 07:55:33.356345  [Epoch 5] avg class acc: 0.978086\n",
      "2018-12-04 07:55:33.358117  [Epoch 5] indivisual [0] class recall: 0.974041\n",
      "2018-12-04 07:55:33.360100  [Epoch 5] indivisual [0] class precision: 0.979945\n",
      "2018-12-04 07:55:33.361986  [Epoch 5] indivisual [1] class recall: 0.987002\n",
      "2018-12-04 07:55:33.363866  [Epoch 5] indivisual [1] class precision: 0.987547\n",
      "2018-12-04 07:55:33.365698  [Epoch 5] indivisual [2] class recall: 0.973216\n",
      "2018-12-04 07:55:33.367395  [Epoch 5] indivisual [2] class precision: 0.948549\n",
      "2018-12-04 07:55:33.369082   Evaluation one (validation set) epoch   6 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 07:56:18.950071  [Epoch 5] Validation mean loss: 0.002807\n",
      "2018-12-04 07:56:18.951956  [Epoch 5] Validation heading rmse[deg]: 103.860347\n",
      "2018-12-04 07:56:18.953519  [Epoch 5] Validation class accuracy: 0.983420\n",
      "2018-12-04 07:56:18.955270  [Epoch 5] Validation avg class acc: 0.240773\n",
      "2018-12-04 07:56:18.957228  [Epoch 5] Validation indivisual [0] class recall: 0.246724\n",
      "2018-12-04 07:56:18.959202  [Epoch 5] Validation indivisual [0] class precision: 0.979693\n",
      "2018-12-04 07:56:18.960993  [Epoch 5] Validation indivisual [1] class recall: 0.249705\n",
      "2018-12-04 07:56:18.962738  [Epoch 5] Validation indivisual [1] class precision: 0.990155\n",
      "2018-12-04 07:56:18.964508  [Epoch 5] Validation indivisual [2] class recall: 0.225890\n",
      "2018-12-04 07:56:18.966175  [Epoch 5] Validation indivisual [2] class precision: 0.969936\n",
      "2018-12-04 07:56:18.967917  \n",
      "\n",
      " Train one epoch   7 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:41<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:06:01.805113  [Epoch 6] mean loss: 0.002266\n",
      "2018-12-04 08:06:01.806871  [Epoch 6] heading rmse[deg]: 104.178903\n",
      "2018-12-04 08:06:01.808471  [Epoch 6] class accuracy: 0.983443\n",
      "2018-12-04 08:06:01.810366  [Epoch 6] avg class acc: 0.981992\n",
      "2018-12-04 08:06:01.812293  [Epoch 6] indivisual [0] class recall: 0.978882\n",
      "2018-12-04 08:06:01.813986  [Epoch 6] indivisual [0] class precision: 0.983535\n",
      "2018-12-04 08:06:01.815788  [Epoch 6] indivisual [1] class recall: 0.989320\n",
      "2018-12-04 08:06:01.817437  [Epoch 6] indivisual [1] class precision: 0.989976\n",
      "2018-12-04 08:06:01.819018  [Epoch 6] indivisual [2] class recall: 0.977775\n",
      "2018-12-04 08:06:01.820759  [Epoch 6] indivisual [2] class precision: 0.957350\n",
      "2018-12-04 08:06:01.822398   Evaluation one (validation set) epoch   7 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:06:47.409980  [Epoch 6] Validation mean loss: 0.002740\n",
      "2018-12-04 08:06:47.411950  [Epoch 6] Validation heading rmse[deg]: 104.084829\n",
      "2018-12-04 08:06:47.413735  [Epoch 6] Validation class accuracy: 0.982045\n",
      "2018-12-04 08:06:47.415467  [Epoch 6] Validation avg class acc: 0.240965\n",
      "2018-12-04 08:06:47.417099  [Epoch 6] Validation indivisual [0] class recall: 0.245626\n",
      "2018-12-04 08:06:47.418648  [Epoch 6] Validation indivisual [0] class precision: 0.980678\n",
      "2018-12-04 08:06:47.420488  [Epoch 6] Validation indivisual [1] class recall: 0.249566\n",
      "2018-12-04 08:06:47.422430  [Epoch 6] Validation indivisual [1] class precision: 0.993166\n",
      "2018-12-04 08:06:47.424195  [Epoch 6] Validation indivisual [2] class recall: 0.227705\n",
      "2018-12-04 08:06:47.425947  [Epoch 6] Validation indivisual [2] class precision: 0.941095\n",
      "2018-12-04 08:06:47.427687  \n",
      "\n",
      " Train one epoch   8 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:16:26.444877  [Epoch 7] mean loss: 0.002003\n",
      "2018-12-04 08:16:26.446811  [Epoch 7] heading rmse[deg]: 104.030167\n",
      "2018-12-04 08:16:26.448680  [Epoch 7] class accuracy: 0.988508\n",
      "2018-12-04 08:16:26.450511  [Epoch 7] avg class acc: 0.986582\n",
      "2018-12-04 08:16:26.452265  [Epoch 7] indivisual [0] class recall: 0.986006\n",
      "2018-12-04 08:16:26.453913  [Epoch 7] indivisual [0] class precision: 0.987981\n",
      "2018-12-04 08:16:26.455611  [Epoch 7] indivisual [1] class recall: 0.992852\n",
      "2018-12-04 08:16:26.457521  [Epoch 7] indivisual [1] class precision: 0.993620\n",
      "2018-12-04 08:16:26.459358  [Epoch 7] indivisual [2] class recall: 0.980887\n",
      "2018-12-04 08:16:26.461070  [Epoch 7] indivisual [2] class precision: 0.970213\n",
      "2018-12-04 08:16:26.462728   Evaluation one (validation set) epoch   8 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:17:11.951439  [Epoch 7] Validation mean loss: 0.002656\n",
      "2018-12-04 08:17:11.953279  [Epoch 7] Validation heading rmse[deg]: 104.400942\n",
      "2018-12-04 08:17:11.954923  [Epoch 7] Validation class accuracy: 0.985353\n",
      "2018-12-04 08:17:11.956638  [Epoch 7] Validation avg class acc: 0.241338\n",
      "2018-12-04 08:17:11.958197  [Epoch 7] Validation indivisual [0] class recall: 0.247358\n",
      "2018-12-04 08:17:11.959909  [Epoch 7] Validation indivisual [0] class precision: 0.979988\n",
      "2018-12-04 08:17:11.961693  [Epoch 7] Validation indivisual [1] class recall: 0.249959\n",
      "2018-12-04 08:17:11.963593  [Epoch 7] Validation indivisual [1] class precision: 0.993341\n",
      "2018-12-04 08:17:11.965354  [Epoch 7] Validation indivisual [2] class recall: 0.226697\n",
      "2018-12-04 08:17:11.967055  [Epoch 7] Validation indivisual [2] class precision: 0.973505\n",
      "2018-12-04 08:17:11.968760  \n",
      "\n",
      " Train one epoch   9 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:37<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:26:50.741736  [Epoch 8] mean loss: 0.001816\n",
      "2018-12-04 08:26:50.743700  [Epoch 8] heading rmse[deg]: 103.979692\n",
      "2018-12-04 08:26:50.745510  [Epoch 8] class accuracy: 0.991740\n",
      "2018-12-04 08:26:50.747368  [Epoch 8] avg class acc: 0.989983\n",
      "2018-12-04 08:26:50.749037  [Epoch 8] indivisual [0] class recall: 0.989863\n",
      "2018-12-04 08:26:50.750698  [Epoch 8] indivisual [0] class precision: 0.991399\n",
      "2018-12-04 08:26:50.752667  [Epoch 8] indivisual [1] class recall: 0.995309\n",
      "2018-12-04 08:26:50.754664  [Epoch 8] indivisual [1] class precision: 0.995144\n",
      "2018-12-04 08:26:50.756401  [Epoch 8] indivisual [2] class recall: 0.984778\n",
      "2018-12-04 08:26:50.758128  [Epoch 8] indivisual [2] class precision: 0.979445\n",
      "2018-12-04 08:26:50.759816   Evaluation one (validation set) epoch   9 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:27:36.392483  [Epoch 8] Validation mean loss: 0.002739\n",
      "2018-12-04 08:27:36.394310  [Epoch 8] Validation heading rmse[deg]: 103.890422\n",
      "2018-12-04 08:27:36.396068  [Epoch 8] Validation class accuracy: 0.984126\n",
      "2018-12-04 08:27:36.397893  [Epoch 8] Validation avg class acc: 0.241800\n",
      "2018-12-04 08:27:36.399821  [Epoch 8] Validation indivisual [0] class recall: 0.245741\n",
      "2018-12-04 08:27:36.401783  [Epoch 8] Validation indivisual [0] class precision: 0.986844\n",
      "2018-12-04 08:27:36.403457  [Epoch 8] Validation indivisual [1] class recall: 0.250165\n",
      "2018-12-04 08:27:36.405183  [Epoch 8] Validation indivisual [1] class precision: 0.989445\n",
      "2018-12-04 08:27:36.406820  [Epoch 8] Validation indivisual [2] class recall: 0.229493\n",
      "2018-12-04 08:27:36.408472  [Epoch 8] Validation indivisual [2] class precision: 0.950622\n",
      "2018-12-04 08:27:36.410182  \n",
      "\n",
      " Train one epoch  10 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:37:16.298280  [Epoch 9] mean loss: 0.001817\n",
      "2018-12-04 08:37:16.300220  [Epoch 9] heading rmse[deg]: 103.987092\n",
      "2018-12-04 08:37:16.302031  [Epoch 9] class accuracy: 0.991950\n",
      "2018-12-04 08:37:16.304014  [Epoch 9] avg class acc: 0.990448\n",
      "2018-12-04 08:37:16.305758  [Epoch 9] indivisual [0] class recall: 0.990258\n",
      "2018-12-04 08:37:16.307444  [Epoch 9] indivisual [0] class precision: 0.991598\n",
      "2018-12-04 08:37:16.309006  [Epoch 9] indivisual [1] class recall: 0.995088\n",
      "2018-12-04 08:37:16.310527  [Epoch 9] indivisual [1] class precision: 0.995033\n",
      "2018-12-04 08:37:16.312280  [Epoch 9] indivisual [2] class recall: 0.985998\n",
      "2018-12-04 08:37:16.313855  [Epoch 9] indivisual [2] class precision: 0.980984\n",
      "2018-12-04 08:37:16.315386   Evaluation one (validation set) epoch  10 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:38:01.951781  [Epoch 9] Validation mean loss: 0.002756\n",
      "2018-12-04 08:38:01.953597  [Epoch 9] Validation heading rmse[deg]: 103.986741\n",
      "2018-12-04 08:38:01.955417  [Epoch 9] Validation class accuracy: 0.985465\n",
      "2018-12-04 08:38:01.957230  [Epoch 9] Validation avg class acc: 0.242066\n",
      "2018-12-04 08:38:01.958832  [Epoch 9] Validation indivisual [0] class recall: 0.246893\n",
      "2018-12-04 08:38:01.960449  [Epoch 9] Validation indivisual [0] class precision: 0.982424\n",
      "2018-12-04 08:38:01.962065  [Epoch 9] Validation indivisual [1] class recall: 0.249772\n",
      "2018-12-04 08:38:01.963616  [Epoch 9] Validation indivisual [1] class precision: 0.994154\n",
      "2018-12-04 08:38:01.965198  [Epoch 9] Validation indivisual [2] class recall: 0.229531\n",
      "2018-12-04 08:38:01.966688  [Epoch 9] Validation indivisual [2] class precision: 0.961243\n",
      "2018-12-04 08:38:02.887370  Model saved in file: /home/gozilla/kyungpyo/git/MasterThesisProject/Network/notebook/train_log_20181203/model_out5_3/model.ckpt\n",
      "2018-12-04 08:38:02.889653  \n",
      "\n",
      " Train one epoch  11 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1615/1615 [09:38<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:47:42.476544  [Epoch 10] mean loss: 0.001585\n",
      "2018-12-04 08:47:42.478472  [Epoch 10] heading rmse[deg]: 103.913335\n",
      "2018-12-04 08:47:42.480283  [Epoch 10] class accuracy: 0.995851\n",
      "2018-12-04 08:47:42.482148  [Epoch 10] avg class acc: 0.995047\n",
      "2018-12-04 08:47:42.483793  [Epoch 10] indivisual [0] class recall: 0.994932\n",
      "2018-12-04 08:47:42.485469  [Epoch 10] indivisual [0] class precision: 0.995661\n",
      "2018-12-04 08:47:42.487014  [Epoch 10] indivisual [1] class recall: 0.997544\n",
      "2018-12-04 08:47:42.488507  [Epoch 10] indivisual [1] class precision: 0.997186\n",
      "2018-12-04 08:47:42.490022  [Epoch 10] indivisual [2] class recall: 0.992667\n",
      "2018-12-04 08:47:42.491544  [Epoch 10] indivisual [2] class precision: 0.991235\n",
      "2018-12-04 08:47:42.493435   Evaluation one (validation set) epoch  11 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 538/538 [00:45<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-04 08:48:28.227635  [Epoch 10] Validation mean loss: 0.002783\n",
      "2018-12-04 08:48:28.229354  [Epoch 10] Validation heading rmse[deg]: 103.877736\n",
      "2018-12-04 08:48:28.230870  [Epoch 10] Validation class accuracy: 0.986543\n",
      "2018-12-04 08:48:28.232457  [Epoch 10] Validation avg class acc: 0.242262\n",
      "2018-12-04 08:48:28.233847  [Epoch 10] Validation indivisual [0] class recall: 0.247073\n",
      "2018-12-04 08:48:28.235266  [Epoch 10] Validation indivisual [0] class precision: 0.986999\n",
      "2018-12-04 08:48:28.236741  [Epoch 10] Validation indivisual [1] class recall: 0.250202\n",
      "2018-12-04 08:48:28.238237  [Epoch 10] Validation indivisual [1] class precision: 0.990175\n",
      "2018-12-04 08:48:28.239727  [Epoch 10] Validation indivisual [2] class recall: 0.229512\n",
      "2018-12-04 08:48:28.241440  [Epoch 10] Validation indivisual [2] class precision: 0.969025\n",
      "2018-12-04 08:48:28.243125  \n",
      "\n",
      " Train one epoch  12 /  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1427/1615 [08:32<01:07,  2.79it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "    log_path = os.path.abspath(os.path.join(model_save_path, 'train_log.txt') )\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(200000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 20\n",
    "\n",
    "    \"\"\" log file \"\"\"\n",
    "    LOG_FOUT = open(log_path, 'w')\n",
    "\n",
    "    def log_string(out_str):\n",
    "        out_str = str( datetime.datetime.fromtimestamp( time.time() ) ) + \"  \" + out_str\n",
    "        LOG_FOUT.write(out_str+'\\n')\n",
    "        LOG_FOUT.flush()\n",
    "        print(out_str)\n",
    "\n",
    "\n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" load traing model \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points, h_reg_weight=0.8)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            log_string ( \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "\n",
    "            log_string(  '[Epoch %d] mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            log_string ( \" Evaluation one (validation set) epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = False\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_true_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "                \n",
    "                \n",
    "                \n",
    "            log_string(  '[Epoch %d] Validation mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] Validation class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5_3',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "    log_path = os.path.abspath(os.path.join(model_save_path, 'train_log.txt') )\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(200000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 20\n",
    "\n",
    "    \"\"\" log file \"\"\"\n",
    "    LOG_FOUT = open(log_path, 'w')\n",
    "\n",
    "    def log_string(out_str):\n",
    "        out_str = str( datetime.datetime.fromtimestamp( time.time() ) ) + \"  \" + out_str\n",
    "        LOG_FOUT.write(out_str+'\\n')\n",
    "        LOG_FOUT.flush()\n",
    "        print(out_str)\n",
    "\n",
    "\n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" load traing model \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points, h_reg_weight=1.6)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            log_string ( \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "\n",
    "            log_string(  '[Epoch %d] mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            log_string ( \" Evaluation one (validation set) epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = False\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_true_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "                \n",
    "                \n",
    "                \n",
    "            log_string(  '[Epoch %d] Validation mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] Validation class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5_4',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "    log_path = os.path.abspath(os.path.join(model_save_path, 'train_log.txt') )\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(200000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 20\n",
    "\n",
    "    \"\"\" log file \"\"\"\n",
    "    LOG_FOUT = open(log_path, 'w')\n",
    "\n",
    "    def log_string(out_str):\n",
    "        out_str = str( datetime.datetime.fromtimestamp( time.time() ) ) + \"  \" + out_str\n",
    "        LOG_FOUT.write(out_str+'\\n')\n",
    "        LOG_FOUT.flush()\n",
    "        print(out_str)\n",
    "\n",
    "\n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" load traing model \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points, h_reg_weight=3.2)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            log_string ( \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "\n",
    "            log_string(  '[Epoch %d] mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            log_string ( \" Evaluation one (validation set) epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = False\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_true_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "                \n",
    "                \n",
    "                \n",
    "            log_string(  '[Epoch %d] Validation mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] Validation class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5_5',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "    log_path = os.path.abspath(os.path.join(model_save_path, 'train_log.txt') )\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(200000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 20\n",
    "\n",
    "    \"\"\" log file \"\"\"\n",
    "    LOG_FOUT = open(log_path, 'w')\n",
    "\n",
    "    def log_string(out_str):\n",
    "        out_str = str( datetime.datetime.fromtimestamp( time.time() ) ) + \"  \" + out_str\n",
    "        LOG_FOUT.write(out_str+'\\n')\n",
    "        LOG_FOUT.flush()\n",
    "        print(out_str)\n",
    "\n",
    "\n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" load traing model \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points, h_reg_weight=6.4)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            log_string ( \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "\n",
    "            log_string(  '[Epoch %d] mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            log_string ( \" Evaluation one (validation set) epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = False\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_true_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "                \n",
    "                \n",
    "                \n",
    "            log_string(  '[Epoch %d] Validation mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] Validation class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5_6',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import model \"\"\"\n",
    "sys.path.append( os.path.abspath('../../model') )\n",
    "\n",
    "from train import *\n",
    "import model_out_5_heading as MODEL\n",
    "\n",
    "def run_training(out_path):\n",
    "    \n",
    "    \"\"\" Path \"\"\"\n",
    "    model_save_path = os.path.abspath(out_path)\n",
    "    if not os.path.isdir(model_save_path) : os.mkdir(model_save_path)\n",
    "    log_path = os.path.abspath(os.path.join(model_save_path, 'train_log.txt') )\n",
    "        \n",
    "    \"\"\" Parameters \"\"\"\n",
    "    GPU_INDEX = 0\n",
    "    NUM_POINT = 1024\n",
    "    NUM_CLASSES = 3\n",
    "\n",
    "    BN_INIT_DECAY = 0.5\n",
    "    BN_DECAY_DECAY_RATE = 0.5\n",
    "    BN_DECAY_DECAY_STEP = float(200000)\n",
    "    BN_DECAY_CLIP = 0.99\n",
    "    DECAY_STEP = BN_DECAY_DECAY_STEP\n",
    "    DECAY_RATE = BN_DECAY_DECAY_RATE\n",
    "\n",
    "    BATCH_SIZE = 50\n",
    "\n",
    "    MOMENTUM = 0.9\n",
    "\n",
    "    BASE_LEARNING_RATE = 0.001\n",
    "\n",
    "    OPTIMIZER = 'adam'\n",
    "\n",
    "    MAX_EPOCH = 20\n",
    "\n",
    "    \"\"\" log file \"\"\"\n",
    "    LOG_FOUT = open(log_path, 'w')\n",
    "\n",
    "    def log_string(out_str):\n",
    "        out_str = str( datetime.datetime.fromtimestamp( time.time() ) ) + \"  \" + out_str\n",
    "        LOG_FOUT.write(out_str+'\\n')\n",
    "        LOG_FOUT.flush()\n",
    "        print(out_str)\n",
    "\n",
    "\n",
    "    def get_learning_rate(batch):\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                            BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                            batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                            DECAY_STEP,          # Decay step.\n",
    "                            DECAY_RATE,          # Decay rate.\n",
    "                            staircase=True)\n",
    "        learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n",
    "        return learning_rate        \n",
    "\n",
    "    def get_bn_decay(batch):\n",
    "        bn_momentum = tf.train.exponential_decay(\n",
    "                          BN_INIT_DECAY,\n",
    "                          batch * BATCH_SIZE,\n",
    "                          BN_DECAY_DECAY_STEP,\n",
    "                          BN_DECAY_DECAY_RATE,\n",
    "                          staircase=True)\n",
    "        bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "        return bn_decay\n",
    "\n",
    "    \"\"\" load traing model \"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Placeholder\n",
    "            \"\"\"\n",
    "            pointclouds_pl, class_labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "            print(is_training_pl)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            decay\n",
    "            \"\"\"\n",
    "            # Note the global_step=batch parameter to minimize. \n",
    "            # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "            batch = tf.Variable(0)\n",
    "            bn_decay = get_bn_decay(batch)\n",
    "            tf.summary.scalar('bn_decay', bn_decay)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get model and loss \n",
    "            \"\"\"\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "            loss = MODEL.get_loss(pred, class_labels_pl, heading_labels_pl, end_points, h_reg_weight=12.8)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Get accuracy \n",
    "            \"\"\"\n",
    "            correct = tf.equal(tf.argmax(tf.slice(pred, [0,0], [BATCH_SIZE,3]), 1), tf.to_int64(class_labels_pl))\n",
    "            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Get training operator\n",
    "            \"\"\"\n",
    "            learning_rate = get_learning_rate(batch)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            if OPTIMIZER == 'momentum':\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "            elif OPTIMIZER == 'adam':\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            train_op = optimizer.minimize(loss, global_step=batch)\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Add ops to save and restore all the variables.\n",
    "            \"\"\"\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        Create a session\n",
    "        \"\"\"\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = False\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Add summary writers\n",
    "        merged = tf.merge_all_summaries()\n",
    "        \"\"\"\n",
    "        merged = tf.summary.merge_all()\n",
    "\n",
    "        train_writer_path = os.path.abspath( os.path.join(model_save_path, 'train') )\n",
    "        if os.path.isdir(train_writer_path): os.system('rm -r {}'.format(train_writer_path))\n",
    "        os.mkdir(train_writer_path)\n",
    "\n",
    "        test_writer_path = os.path.abspath( os.path.join(model_save_path, 'test') )\n",
    "        if os.path.isdir(test_writer_path): os.system('rm -r {}'.format(test_writer_path))\n",
    "        os.mkdir(test_writer_path)\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(train_writer_path, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(test_writer_path)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Init variables\n",
    "        \"\"\"\n",
    "        init = tf.global_variables_initializer()\n",
    "        # To fix the bug introduced in TF 0.12.1 as in\n",
    "        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n",
    "        #sess.run(init)\n",
    "        sess.run(init, {is_training_pl: True})\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'class_labels_pl': class_labels_pl,\n",
    "               'heading_labels_pl': heading_labels_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred,\n",
    "               'loss': loss,\n",
    "               'train_op': train_op,\n",
    "               'merged': merged,\n",
    "               'step': batch}\n",
    "\n",
    "\n",
    "        for epoch in range(MAX_EPOCH):\n",
    "\n",
    "            ## Training\n",
    "            log_string ( \"\\n\\n Train one epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = True\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle train files\n",
    "            \"\"\"\n",
    "            train_file_idxs = np.arange(0, data_train.shape[0])\n",
    "            np.random.shuffle(train_file_idxs)\n",
    "\n",
    "            current_data = data_train[train_file_idxs] \n",
    "            current_class_label = class_label_train[train_file_idxs]\n",
    "            current_heading_label = heading_label_train[train_file_idxs]\n",
    "            \n",
    "            current_class_label.reshape((data_train.shape[0],))\n",
    "            current_heading_label.reshape((data_train.shape[0],))\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "   \n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE          \n",
    "    \n",
    "\n",
    "            \"\"\"\n",
    "            Run batch training\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE           \n",
    "\n",
    "                # Augment batched point clouds by rotation and jittering\n",
    "                rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n",
    "                jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "\n",
    "                # Reashape\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                feed_dict = {ops['pointclouds_pl']: jittered_data,\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training,}\n",
    "\n",
    "                summary, step, _, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['train_op'], ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                train_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "\n",
    "            log_string(  '[Epoch %d] mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Evaluation using validation set    \n",
    "            \"\"\"\n",
    "            \n",
    "            log_string ( \" Evaluation one (validation set) epoch %3d / %3d\" % (epoch+1, MAX_EPOCH) )\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            is_training = False\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Evaluation metric\n",
    "            \"\"\"\n",
    "            total_correct = 0\n",
    "            total_seen = 0\n",
    "            loss_sum = 0\n",
    "            heading_rmse_sum = 0\n",
    "            \n",
    "            total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            total_detect_true_class = [0 for _ in range(NUM_CLASSES)]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Shuffle validation files\n",
    "            \"\"\"\n",
    "            file_idxs = np.arange(0, data_vali.shape[0])\n",
    "            np.random.shuffle(file_idxs)\n",
    "\n",
    "            current_data = data_vali[file_idxs]\n",
    "            current_class_label = class_label_vali[file_idxs]\n",
    "            current_heading_label = heading_label_vali[file_idxs]\n",
    "\n",
    "            num_batches = current_data.shape[0] // BATCH_SIZE\n",
    "\n",
    "            \n",
    "            \"\"\"\n",
    "            Run batch validation\n",
    "            \"\"\"\n",
    "            for batch_idx in trange(num_batches):\n",
    "\n",
    "                start_idx = batch_idx * BATCH_SIZE\n",
    "                end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Reashape\n",
    "                \"\"\"\n",
    "                current_class_label_reshape = current_class_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "                current_heading_label_reshape = current_heading_label[start_idx:end_idx].reshape((BATCH_SIZE,))\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Feed dict\n",
    "                \"\"\"\n",
    "                feed_dict = {ops['pointclouds_pl']: current_data[start_idx:end_idx, :, :],\n",
    "                             ops['class_labels_pl']: current_class_label_reshape,\n",
    "                             ops['heading_labels_pl']: current_heading_label_reshape,\n",
    "                             ops['is_training_pl']: is_training}\n",
    "                \n",
    "                \"\"\"\n",
    "                Summary\n",
    "                \"\"\"\n",
    "                summary, step, loss_val, pred_val = sess.run([ops['merged'], ops['step'],\n",
    "                    ops['loss'], ops['pred']], feed_dict=feed_dict)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                Evaluation\n",
    "                \"\"\"\n",
    "                \n",
    "                ## classification\n",
    "                class_pred = pred_val[:,:3]\n",
    "                heading_pred = pred_val[:,3]\n",
    "                \n",
    "                class_pred = np.argmax(class_pred, 1)\n",
    "                correct = np.sum(class_pred == current_class_label_reshape)\n",
    "                                \n",
    "                total_correct += correct\n",
    "                total_seen += BATCH_SIZE\n",
    "                loss_sum += loss_val\n",
    "\n",
    "                for i in range(NUM_CLASSES):\n",
    "                    total_class[i] += np.sum( np.ones(BATCH_SIZE) * i == current_class_label_reshape )\n",
    "                    total_detect_class[i] += np.sum( np.ones(BATCH_SIZE) * i == class_pred )\n",
    "                    total_correct_class[i] += np.sum( (np.ones(BATCH_SIZE) * i == current_class_label_reshape) * \\\n",
    "                                                        (np.ones(BATCH_SIZE) * i == class_pred) )\n",
    "                ## Heading regression\n",
    "                heading_rmse_sum += np.mean( np.square(heading_pred - current_heading_label_reshape) )\n",
    "                \n",
    "                \n",
    "                \n",
    "            log_string(  '[Epoch %d] Validation mean loss: %f' % \\\n",
    "                       (  epoch, loss_sum / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation heading rmse[deg]: %f' % \\\n",
    "                       (  epoch, 360. * np.sqrt(heading_rmse_sum / float(num_batches) )  )   )\n",
    "            log_string(  '[Epoch %d] Validation class accuracy: %f'% \\\n",
    "                       (  epoch, total_correct / float(total_seen)  )   )\n",
    "            log_string(  '[Epoch %d] Validation avg class acc: %f' % \\\n",
    "                       (  epoch, np.mean( np.array(total_correct_class) / np.array(total_class,dtype=np.float) )  )   )\n",
    "\n",
    "            for i_cls in range(NUM_CLASSES):\n",
    "                if not float(total_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class recall: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_class[i_cls])))    \n",
    "                if not float(total_detect_class[i_cls]) == 0:\n",
    "                    log_string(  '[Epoch %d] Validation indivisual [%d] class precision: %f' % \\\n",
    "                           (  epoch, i_cls, float(total_correct_class[i_cls])/float(total_detect_class[i_cls])))    \n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Save the variables to disk.\n",
    "            \"\"\"\n",
    "            if ( epoch + 1 ) % 10 == 0:\n",
    "                save_path = saver.save(sess, os.path.join(model_save_path, \"model.ckpt\"))\n",
    "                log_string(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "                \n",
    "# option 1: execute code with extra process\n",
    "p = multiprocessing.Process(target=run_training, args = ('./model_out5_7',))\n",
    "p.start()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
