{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of accumulated point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/media/kyungpyo/SmartCarContest/MasterThesisProject/KITTI_Evaluation/notebook/parsed_kitti_data_181212.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f8a945aafc9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mparsed_kitti_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/media/kyungpyo/SmartCarContest/MasterThesisProject/KITTI_Evaluation/notebook/parsed_kitti_data_181212.pickle'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load dataset\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data_path = os.path.abspath(\"parsed_kitti_data_181212.pickle\")\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(data_path, 'rb') as f:\n",
    "    parsed_kitti_data = pickle.load(f)\n",
    "\n",
    "print \"Dataset was loaded\"\n",
    "\n",
    "\"\"\"\n",
    "Data structure\n",
    "\"\"\"\n",
    "\n",
    "for date in parsed_kitti_data:\n",
    "    print \"date:\", date\n",
    "    \n",
    "    for drive in parsed_kitti_data[date]:\n",
    "        print \"\\tdrive:\", drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 2011_09_26\n",
      "\tdrive: 0002\n",
      "\t\ttrack:    rot   trans   type   rect   pc 11\n",
      "\t\ttrack:    rot   trans   type   rect   pc 28\n",
      "\t\ttrack:    rot   trans   type   rect   pc 16\n",
      "\n",
      "\n",
      "Saved new dataset, protocol=2, parsed_kitti_data_181212_0002.pickle\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "Execution time: 0.107073068619 \n",
      "=======================================================\n",
      "=======================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "\n",
    "\n",
    "parsed_kitti_data_w_r = {}\n",
    "\n",
    "for date in parsed_kitti_data:\n",
    "    print \"date:\", date\n",
    "    parsed_kitti_data_w_r[date] = {}\n",
    "    \n",
    "    for drive in parsed_kitti_data[date]:\n",
    "                \n",
    "        print \"\\tdrive:\", drive\n",
    "        \n",
    "        parsed_kitti_data_w_r_drive = copy.deepcopy(parsed_kitti_data_w_r)\n",
    "        \n",
    "        parsed_kitti_data_w_r_drive[date][drive] = []\n",
    "        \n",
    "        for tracklets_list in parsed_kitti_data[date][drive]:\n",
    "            \n",
    "            tracklets_list_w_r = {}\n",
    "            \n",
    "            print \"\\t\\ttrack: \",\n",
    "            for key in tracklets_list:\n",
    "                \n",
    "                print \" \", key,\n",
    "                tracklets_list_w_r[key] = tracklets_list[key]\n",
    "                \n",
    "            print \"{}\".format(len(tracklets_list[key]))\n",
    "            \n",
    "            parsed_kitti_data_w_r_drive[date][drive].append(tracklets_list_w_r)\n",
    "            \n",
    "        \"\"\" \n",
    "        Save pickle\n",
    "        \"\"\"\n",
    "        out_file_nme = \"parsed_kitti_data_181212_{}.pickle\".format(drive)\n",
    "        with open(out_file_nme, 'wb') as f:\n",
    "            pickle.dump(parsed_kitti_data_w_r_drive, f, protocol=2)\n",
    "\n",
    "        print(\"\\n\\nSaved new dataset, protocol=2, {}\\n\\n\".format(out_file_nme))\n",
    "        del parsed_kitti_data_w_r_drive\n",
    "            \n",
    "\n",
    "print \"\\n\\n=======================================================\"\n",
    "print \"=======================================================\"        \n",
    "print \"Execution time: {} \".format( time.time() - start_time )\n",
    "print \"=======================================================\"\n",
    "print \"=======================================================\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found:  parsed_kitti_data_181212_0002 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\"\"\"\n",
    "Search all files \n",
    "\"\"\"\n",
    "basedir = './'\n",
    "p = re.compile('parsed_kitti_data_181212_\\d{4}')\n",
    "\n",
    "drives = []\n",
    "\n",
    "cnt = 0\n",
    "for root,d_names,f_names in os.walk(basedir):\n",
    "    for f in f_names:\n",
    "        m = p.match(f)\n",
    "        if m:\n",
    "            cnt +=1\n",
    "            print 'Match found: ', m.group(), cnt\n",
    "            drives.append(m.group() + \".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 2011_09_26\n",
      "\tdrive: 0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [00:16<00:08,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 3/3 [00:24<00:00,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Saved new dataset, protocol=2\n",
      "\n",
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "Execution time: 24.9234850407 \n",
      "=======================================================\n",
      "=======================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "\n",
    "from registration import *\n",
    "\n",
    "\n",
    "for d in drives:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    with open(d, 'rb') as f:\n",
    "        parsed_kitti_data_w_r = pickle.load(f)\n",
    "    \n",
    "\n",
    "    for date in parsed_kitti_data_w_r:\n",
    "        print \"date:\", date\n",
    "        \n",
    "        assert len(parsed_kitti_data_w_r[date]) == 1\n",
    "\n",
    "        for drive in parsed_kitti_data_w_r[date]:\n",
    "\n",
    "            print \"\\tdrive:\", drive\n",
    "\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            for i in tqdm.trange( len(parsed_kitti_data_w_r[date][drive]) ):\n",
    "\n",
    "                pc = parsed_kitti_data_w_r[date][drive][i]['pc']\n",
    "\n",
    "                \"\"\"\n",
    "                Registration\n",
    "                \"\"\"\n",
    "                parsed_kitti_data_w_r[date][drive][i]['pc_wr'] = []\n",
    "                sys.stdout.flush()\n",
    "                if len(pc) > 1:\n",
    "                    parsed_kitti_data_w_r[date][drive][i]['pc_wr'] = registration_layered(pc)\n",
    "                else:\n",
    "                    parsed_kitti_data_w_r[date][drive][i]['pc_wr'] = []\n",
    "\n",
    "                    print \"\\tdrive:\", drive\n",
    "                    \n",
    "    \"\"\"\n",
    "    Save data\n",
    "    \"\"\"\n",
    "    out_data_path = 'parsed_kitti_data_w_r_181212_{}.pickle'.format(drive)\n",
    "    \n",
    "    with open(out_data_path, 'wb') as f:\n",
    "        pickle.dump(parsed_kitti_data_w_r, f, protocol=2)\n",
    "    \n",
    "    print(\"\\n\\nSaved new dataset, protocol=2\")\n",
    "    \n",
    "print \"\\n\\n=======================================================\"\n",
    "print \"=======================================================\"        \n",
    "print \"Execution time: {} \".format( time.time() - start_time )\n",
    "print \"=======================================================\"\n",
    "print \"=======================================================\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found:  parsed_kitti_data_w_r_181212_0002 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\"\"\"\n",
    "Search all files \n",
    "\"\"\"\n",
    "basedir = './'\n",
    "p = re.compile('parsed_kitti_data_w_r_181212_\\d{4}')\n",
    "\n",
    "drives = []\n",
    "\n",
    "cnt = 0\n",
    "for root,d_names,f_names in os.walk(basedir):\n",
    "    for f in f_names:\n",
    "        m = p.match(f)\n",
    "        if m:\n",
    "            cnt +=1\n",
    "            print 'Match found: ', m.group(), cnt\n",
    "            drives.append(m.group() + \".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "INFO:tensorflow:Restoring parameters from /media/kyungpyo/SmartCarContest/MasterThesisProject/Network/notebook/train_log_20181208/model_out_1/model009.ckpt\n",
      "Model restored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date: 2011_09_26\n",
      "\tdrive: 0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 26.60it/s]\n",
      "100%|██████████| 28/28 [00:01<00:00, 25.97it/s]\n",
      "100%|██████████| 28/28 [00:01<00:00, 21.13it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 25.94it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 26.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Saved new dataset, protocol=2\n",
      "\n",
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "Execution time: 32.107899189 \n",
      "=======================================================\n",
      "=======================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sys\n",
    "import pickle\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from registration import *\n",
    "\n",
    "\"\"\" \n",
    "Import model \n",
    "\"\"\"\n",
    "sys.path.append( os.path.abspath('../../Network/model') )\n",
    "from train import *\n",
    "import model_out_5_heading_3_relu as MODEL\n",
    "model_path = os.path.abspath('../../Network/notebook/train_log_20181208/model_out_1/model009.ckpt')\n",
    "\n",
    "def NormalizeResample(data, num_sample):\n",
    "    \"\"\" data is in N x ...\n",
    "    we want to keep num_samplexC of them.\n",
    "    if N > num_sample, we will randomly keep num_sample of them.\n",
    "    if N < num_sample, we will randomly duplicate samples.\n",
    "    \"\"\"\n",
    "  \n",
    "\n",
    "    ## normalizing    \n",
    "    min_val = np.min(data, axis = 0)\n",
    "    \n",
    "    data[:,0] = data[:,0] - min_val[0]\n",
    "    data[:,1] = data[:,1] - min_val[1]\n",
    "    data[:,2] = data[:,2] - min_val[2]\n",
    "      \n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    data[:,0] = data[:,0] / max_val\n",
    "    data[:,1] = data[:,1] / max_val\n",
    "    data[:,2] = data[:,2] / max_val\n",
    "                 \n",
    "        \n",
    "    ## resampling\n",
    "    N = data.shape[0]\n",
    "    if (N == num_sample):\n",
    "        return data\n",
    "    elif (N > num_sample):\n",
    "        sample = np.random.choice(N, num_sample)\n",
    "        return data[sample, ...]\n",
    "    else:\n",
    "        sample = np.random.choice(N, num_sample-N)\n",
    "        dup_data = data[sample, ...]\n",
    "        return np.concatenate([data, dup_data], 0)\n",
    "\n",
    "for d in drives:\n",
    "    \n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    with open(d, 'rb') as f:\n",
    "        parsed_kitti_data_w_r = pickle.load(f)\n",
    "\n",
    "        \"\"\"\n",
    "        Classification\n",
    "        \"\"\"\n",
    "\n",
    "        is_training = False\n",
    "\n",
    "        GPU_INDEX = 0\n",
    "        NUM_POINT = 1024\n",
    "        NUM_CLASSES = 3\n",
    "        BATCH_SIZE = 1\n",
    "\n",
    "        # reset graph\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "            pointclouds_pl, labels_pl, heading_labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "            is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "            # simple model\n",
    "            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl)\n",
    "\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        config.allow_soft_placement = True\n",
    "        config.log_device_placement = True\n",
    "\n",
    "        ops = {'pointclouds_pl': pointclouds_pl,\n",
    "               'is_training_pl': is_training_pl,\n",
    "               'pred': pred}\n",
    "\n",
    "        # ops.reset_default_graph() \n",
    "\n",
    "        sess = tf.Session(config=config)\n",
    "\n",
    "        # Restore variables from disk.\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        print \"Model restored\"\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "        for date in parsed_kitti_data_w_r:\n",
    "            print \"date:\", date\n",
    "\n",
    "            for drive in parsed_kitti_data_w_r[date]:\n",
    "                print \"\\tdrive:\", drive\n",
    "\n",
    "                for i in range( len(parsed_kitti_data_w_r[date][drive]) ):\n",
    "\n",
    "                    clusters = parsed_kitti_data_w_r[date][drive][i]['pc_wr']\n",
    "                    parsed_kitti_data_w_r[date][drive][i]['cls_wr'] = []\n",
    "                    for j in tqdm.trange(len(clusters)):\n",
    "\n",
    "                        if len(clusters[j]) == 0:\n",
    "                            continue\n",
    "\n",
    "                        current_data = NormalizeResample(clusters[j], NUM_POINT)\n",
    "                        feed_dict = {ops['pointclouds_pl']: current_data.reshape((BATCH_SIZE, NUM_POINT, 3)),\n",
    "                                     ops['is_training_pl']: is_training}\n",
    "                        pred_val = sess.run([ops['pred']], feed_dict=feed_dict )\n",
    "\n",
    "                        cls = np.argmax(pred_val[0][:3], 1)\n",
    "                        parsed_kitti_data_w_r[date][drive][i]['cls_wr'].append(cls)\n",
    "\n",
    "                    clusters = parsed_kitti_data_w_r[date][drive][i]['pc']\n",
    "                    parsed_kitti_data_w_r[date][drive][i]['cls'] = []\n",
    "                    for j in tqdm.trange(len(clusters)):\n",
    "\n",
    "                        if len(clusters[j]) == 0:\n",
    "                            continue\n",
    "                        current_data = NormalizeResample(clusters[j][:,:3], NUM_POINT)\n",
    "                        feed_dict = {ops['pointclouds_pl']: current_data.reshape((BATCH_SIZE, NUM_POINT, 3)),\n",
    "                                     ops['is_training_pl']: is_training}\n",
    "                        pred_val = sess.run([ops['pred']], feed_dict=feed_dict )\n",
    "\n",
    "                        cls = np.argmax(pred_val[0][:3], 1)\n",
    "                        parsed_kitti_data_w_r[date][drive][i]['cls'].append(cls)\n",
    "                    \n",
    "    \"\"\"\n",
    "    Save data\n",
    "    \"\"\"\n",
    "    out_data_path = 'parsed_kitti_data_w_r_c_181212_{}.pickle'.format(drive)\n",
    "    \n",
    "    with open(out_data_path, 'wb') as f:\n",
    "        pickle.dump(parsed_kitti_data_w_r, f, protocol=2)\n",
    "    \n",
    "    print(\"\\n\\nSaved new dataset, protocol=2\")\n",
    "    \n",
    "    \n",
    "print \"\\n\\n=======================================================\"\n",
    "print \"=======================================================\"        \n",
    "print \"Execution time: {} \".format( time.time() - start_time )\n",
    "print \"=======================================================\"\n",
    "print \"=======================================================\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
